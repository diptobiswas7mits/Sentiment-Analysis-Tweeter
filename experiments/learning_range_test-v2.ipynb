{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.autonotebook import tqdm\n",
    "#import utils\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    MAX_LEN = 192\n",
    "    TRAIN_BATCH_SIZE = 1\n",
    "    VALID_BATCH_SIZE = 1\n",
    "    GRAD_ACC_STEPS = 1\n",
    "    EPOCHS = 5\n",
    "    LEARNING_RATE = 3e-5\n",
    "    DATA_DIR = Path('')\n",
    "    MODEL_NAME = \"roberta-base\"\n",
    "    TRAINING_FILE = \"train_folds.csv\"\n",
    "    TOKENIZER = tokenizers.ByteLevelBPETokenizer( ##explore this\n",
    "        vocab_file=f\"vocab.json\", \n",
    "        merges_file=f\"merges.txt\", \n",
    "        lowercase=True,\n",
    "        add_prefix_space=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetModel(transformers.BertPreTrainedModel):\n",
    "    def __init__(self, conf):\n",
    "        super(TweetModel, self).__init__(conf)\n",
    "        self.roberta = transformers.RobertaModel.from_pretrained(config.MODEL_NAME, config=conf)\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.conv1d_128_0 = nn.Conv1d(768, 128, kernel_size=2, \\\n",
    "                           stride=1, padding=0, dilation=1, groups=1, \\\n",
    "                           bias=True, padding_mode='zeros')\n",
    "        self.conv1d_128_1 = nn.Conv1d(768, 128, kernel_size=2, \\\n",
    "                           stride=1, padding=0, dilation=1, groups=1, \\\n",
    "                           bias=True, padding_mode='zeros')\n",
    "        self.conv1d_64_0 = nn.Conv1d(128, 64, kernel_size=2, \\\n",
    "                           stride=1, padding=0, dilation=1, groups=1, \\\n",
    "                           bias=True, padding_mode='zeros')\n",
    "        self.conv1d_64_1 = nn.Conv1d(128, 64, kernel_size=2, \\\n",
    "                           stride=1, padding=0, dilation=1, groups=1, \\\n",
    "                           bias=True, padding_mode='zeros')\n",
    "        \n",
    "        self.relu = nn.LeakyReLU()\n",
    "        \n",
    "        self.l0 = nn.Linear(64, 1)\n",
    "        self.l1 = nn.Linear(64, 1)\n",
    "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
    "        torch.nn.init.normal_(self.l1.weight, std=0.02)\n",
    "        \n",
    "        self.pad = nn.ConstantPad1d((0, 1), 0)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        out, _ = self.roberta( #\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        #out =  torch.cat((out[-1], out[-2]), dim=-1)\n",
    "        out1 = out.permute(0, 2, 1)\n",
    "        out1 = self.drop_out(out1)\n",
    "        #print(out.shape)\n",
    "        out1 = self.pad(out1)\n",
    "        #print(out.shape)\n",
    "        out1 = self.conv1d_128_0(out1)\n",
    "        #print(out.shape)\n",
    "        out1 = self.relu(out1)\n",
    "        #print(out.shape)\n",
    "        out1 = self.pad(out1)\n",
    "        out1 = self.conv1d_64_0(out1)\n",
    "        #print(out.shape)\n",
    "        #out = out.flatten()\n",
    "        out1 = out1.permute(0, 2, 1)\n",
    "        #print(out.shape)\n",
    "        start_logits = self.l0(out1)\n",
    "        #print(logits.shape)\n",
    "        \n",
    "        #out =  torch.cat((out[-1], out[-2]), dim=-1)\n",
    "        out2 = out.permute(0, 2, 1)\n",
    "        out2 = self.drop_out(out2)\n",
    "        #print(out.shape)\n",
    "        out2 = self.pad(out2)\n",
    "        #print(out.shape)\n",
    "        out2 = self.conv1d_128_1(out2)\n",
    "        #print(out.shape)\n",
    "        out2 = self.relu(out2)\n",
    "        #print(out.shape)\n",
    "        out2 = self.pad(out2)\n",
    "        out2 = self.conv1d_64_1(out2)\n",
    "        #print(out.shape)\n",
    "        #out = out.flatten()\n",
    "        out2 = out2.permute(0, 2, 1)\n",
    "        #print(out.shape)\n",
    "        end_logits = self.l1(out2)\n",
    "        #print(logits.shape)\n",
    "        \n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        return (start_logits, end_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
    "    tweet = \" \" + \" \".join(str(tweet).split())\n",
    "    selected_text = \" \" + \" \".join(str(selected_text).split())\n",
    "\n",
    "    len_st = len(selected_text) - 1\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st - 1\n",
    "            break\n",
    "\n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 != None and idx1 != None:\n",
    "        for ct in range(idx0, idx1 + 1):\n",
    "            char_targets[ct] = 1\n",
    "    \n",
    "    tok_tweet = tokenizer.encode(tweet)\n",
    "    input_ids_orig = tok_tweet.ids\n",
    "    tweet_offsets = tok_tweet.offsets\n",
    "    \n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
    "        if sum(char_targets[offset1: offset2]) > 0:\n",
    "            target_idx.append(j)\n",
    "    \n",
    "    targets_start = target_idx[0]\n",
    "    targets_end = target_idx[-1]\n",
    "\n",
    "    sentiment_id = {\n",
    "        'positive': 1313,\n",
    "        'negative': 2430,\n",
    "        'neutral': 7974\n",
    "    }\n",
    "    \n",
    "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
    "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n",
    "    mask = [1] * len(token_type_ids)\n",
    "    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
    "    targets_start += 4\n",
    "    targets_end += 4\n",
    "\n",
    "    padding_length = max_len - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([1] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
    "    \n",
    "    return {\n",
    "        'ids': input_ids,\n",
    "        'mask': mask,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'targets_start': targets_start,\n",
    "        'targets_end': targets_end,\n",
    "        'orig_tweet': tweet,\n",
    "        'orig_selected': selected_text,\n",
    "        'sentiment': sentiment,\n",
    "        'offsets': tweet_offsets\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TweetDataset:\n",
    "    def __init__(self, tweet, sentiment, selected_text):\n",
    "        self.tweet = tweet\n",
    "        self.sentiment = sentiment\n",
    "        self.selected_text = selected_text\n",
    "        self.tokenizer = config.TOKENIZER\n",
    "        self.max_len = config.MAX_LEN\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = process_data(\n",
    "            self.tweet[item], \n",
    "            self.selected_text[item], \n",
    "            self.sentiment[item],\n",
    "            self.tokenizer,\n",
    "            self.max_len\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
    "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
    "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
    "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
    "            'orig_tweet': data[\"orig_tweet\"],\n",
    "            'orig_selected': data[\"orig_selected\"],\n",
    "            'sentiment': data[\"sentiment\"],\n",
    "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def calculate_jaccard_score(\n",
    "    original_tweet, \n",
    "    target_string, \n",
    "    sentiment_val, \n",
    "    idx_start, \n",
    "    idx_end, \n",
    "    offsets,\n",
    "    verbose=False):\n",
    "    \n",
    "    if idx_end < idx_start:\n",
    "        idx_end = idx_start\n",
    "    \n",
    "    filtered_output  = \"\"\n",
    "    for ix in range(idx_start, idx_end + 1):\n",
    "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
    "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
    "            filtered_output += \" \"\n",
    "\n",
    "    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n",
    "        filtered_output = original_tweet\n",
    "\n",
    "    jac = jaccard(target_string.strip(), filtered_output.strip())\n",
    "    return jac, filtered_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value.\n",
    "\n",
    "    Examples::\n",
    "        >>> # Initialize a meter to record loss\n",
    "        >>> losses = AverageMeter()\n",
    "        >>> # Update meter after every minibatch update\n",
    "        >>> losses.update(loss_value, batch_size)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, model_path):\n",
    "\n",
    "        score = val_loss#-val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, model_path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, model_path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, model_path):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(logits, positions):\n",
    "    start_logits, end_logits = logits\n",
    "    start_positions, end_positions = positions\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    start_loss = loss_fct(start_logits, start_positions)\n",
    "    end_loss = loss_fct(end_logits, end_positions)\n",
    "    total_loss = (start_loss + end_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    jaccards = AverageMeter()\n",
    "\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    \n",
    "    for bi, d in enumerate(tk0):\n",
    "\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        sentiment = d[\"sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        offsets = d[\"offsets\"]\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets_start = targets_start.to(device, dtype=torch.long)\n",
    "        targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs_start, outputs_end = model(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
    "        loss = loss / config.GRAD_ACC_STEPS\n",
    "        loss.backward()\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "        if (bi+1) % config.GRAD_ACC_STEPS == 0:\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step() \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "        jaccard_scores = []\n",
    "        for px, tweet in enumerate(orig_tweet):\n",
    "            selected_tweet = orig_selected[px]\n",
    "            tweet_sentiment = sentiment[px]\n",
    "            jaccard_score, _ = calculate_jaccard_score(\n",
    "                original_tweet=tweet,\n",
    "                target_string=selected_tweet,\n",
    "                sentiment_val=tweet_sentiment,\n",
    "                idx_start=np.argmax(outputs_start[px, :]),\n",
    "                idx_end=np.argmax(outputs_end[px, :]),\n",
    "                offsets=offsets[px]\n",
    "            )\n",
    "            jaccard_scores.append(jaccard_score)\n",
    "\n",
    "        jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "        losses.update(loss.item(), ids.size(0))\n",
    "        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def eval_fn(data_loader, model, device):\n",
    "#     model.eval()\n",
    "#     losses = AverageMeter()\n",
    "#     jaccards = AverageMeter()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "#         for bi, d in enumerate(tk0):\n",
    "#             ids = d[\"ids\"]\n",
    "#             token_type_ids = d[\"token_type_ids\"]\n",
    "#             mask = d[\"mask\"]\n",
    "#             sentiment = d[\"sentiment\"]\n",
    "#             orig_selected = d[\"orig_selected\"]\n",
    "#             orig_tweet = d[\"orig_tweet\"]\n",
    "#             targets_start = d[\"targets_start\"]\n",
    "#             targets_end = d[\"targets_end\"]\n",
    "#             offsets = d[\"offsets\"].numpy()\n",
    "\n",
    "#             ids = ids.to(device, dtype=torch.long)\n",
    "#             token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "#             mask = mask.to(device, dtype=torch.long)\n",
    "#             targets_start = targets_start.to(device, dtype=torch.long)\n",
    "#             targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "#             outputs_start, outputs_end = model(\n",
    "#                 ids=ids,\n",
    "#                 mask=mask,\n",
    "#                 token_type_ids=token_type_ids\n",
    "#             )\n",
    "#             loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
    "#             outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "#             outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "#             jaccard_scores = []\n",
    "#             for px, tweet in enumerate(orig_tweet):\n",
    "#                 selected_tweet = orig_selected[px]\n",
    "#                 tweet_sentiment = sentiment[px]\n",
    "#                 jaccard_score, _ = calculate_jaccard_score(\n",
    "#                     original_tweet=tweet,\n",
    "#                     target_string=selected_tweet,\n",
    "#                     sentiment_val=tweet_sentiment,\n",
    "#                     idx_start=np.argmax(outputs_start[px, :]),\n",
    "#                     idx_end=np.argmax(outputs_end[px, :]),\n",
    "#                     offsets=offsets[px]\n",
    "#                 )\n",
    "#                 jaccard_scores.append(jaccard_score)\n",
    "\n",
    "#             jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "#             losses.update(loss.item(), ids.size(0))\n",
    "#             tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n",
    "    \n",
    "#     print(f\"Jaccard = {jaccards.avg}\")\n",
    "#     return jaccards.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def run(fold):\n",
    "#     dfx = pd.read_csv(config.TRAINING_FILE)\n",
    "\n",
    "#     df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n",
    "#     df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
    "# #     df_train = df_train.head(100)\n",
    "# #     df_valid = df_train\n",
    "# #     #\n",
    "    \n",
    "#     train_dataset = TweetDataset(\n",
    "#         tweet=df_train.text.values,\n",
    "#         sentiment=df_train.sentiment.values,\n",
    "#         selected_text=df_train.selected_text.values\n",
    "#     )\n",
    "\n",
    "#     train_data_loader = torch.utils.data.DataLoader(\n",
    "#         train_dataset,\n",
    "#         batch_size=config.TRAIN_BATCH_SIZE,\n",
    "#         num_workers=4\n",
    "#     )\n",
    "\n",
    "#     valid_dataset = TweetDataset(\n",
    "#         tweet=df_valid.text.values,\n",
    "#         sentiment=df_valid.sentiment.values,\n",
    "#         selected_text=df_valid.selected_text.values\n",
    "#     )\n",
    "\n",
    "#     valid_data_loader = torch.utils.data.DataLoader(\n",
    "#         valid_dataset,\n",
    "#         batch_size=config.VALID_BATCH_SIZE,\n",
    "#         num_workers=2\n",
    "#     )\n",
    "\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     model_config = transformers.RobertaConfig.from_pretrained(config.MODEL_NAME)\n",
    "#     #model_config.output_hidden_states = True\n",
    "#     model = TweetModel(conf=model_config)\n",
    "#     model.to(device)\n",
    "\n",
    "#     num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "#     param_optimizer = list(model.named_parameters())\n",
    "#     no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "#     optimizer_parameters = [\n",
    "#         {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "#         {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "#     ]\n",
    "#     optimizer = AdamW(optimizer_parameters, lr=config.LEARNING_RATE)\n",
    "#     scheduler = get_linear_schedule_with_warmup(\n",
    "#         optimizer, \n",
    "#         num_warmup_steps=0, \n",
    "#         num_training_steps=num_train_steps\n",
    "#     )\n",
    "\n",
    "#     es = EarlyStopping(patience=100)#, mode=\"max\")\n",
    "#     print(f\"Training is Starting for fold={fold}\")\n",
    "    \n",
    "#     # I'm training only for 3 epochs even though I specified 5!!!\n",
    "#     for epoch in range(config.EPOCHS):\n",
    "#         train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n",
    "#         jaccard = eval_fn(valid_data_loader, model, device)\n",
    "#         print(f\"Jaccard Score = {jaccard}\")\n",
    "#         es(jaccard, model, model_path=f\"model_{fold}-exp3.bin\")\n",
    "#         if es.early_stop:\n",
    "#             print(\"Early stopping\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import torch\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "\n",
    "    IS_AMP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    import logging\n",
    "\n",
    "    logging.basicConfig()\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.warning(\n",
    "        \"To enable mixed precision training, please install `apex`. \"\n",
    "        \"Or you can re-install this package by the following command:\\n\"\n",
    "        '  pip install torch-lr-finder -v --global-option=\"amp\"'\n",
    "    )\n",
    "    IS_AMP_AVAILABLE = False\n",
    "    del logging\n",
    "\n",
    "\n",
    "class LRFinder(object):\n",
    "    \"\"\"Learning rate range test.\n",
    "    The learning rate range test increases the learning rate in a pre-training run\n",
    "    between two boundaries in a linear or exponential manner. It provides valuable\n",
    "    information on how well the network can be trained over a range of learning rates\n",
    "    and what is the optimal learning rate.\n",
    "    Arguments:\n",
    "        model (torch.nn.Module): wrapped model.\n",
    "        optimizer (torch.optim.Optimizer): wrapped optimizer where the defined learning\n",
    "            is assumed to be the lower boundary of the range test.\n",
    "        criterion (torch.nn.Module): wrapped loss function.\n",
    "        device (str or torch.device, optional): a string (\"cpu\" or \"cuda\") with an\n",
    "            optional ordinal for the device type (e.g. \"cuda:X\", where is the ordinal).\n",
    "            Alternatively, can be an object representing the device on which the\n",
    "            computation will take place. Default: None, uses the same device as `model`.\n",
    "        memory_cache (boolean, optional): if this flag is set to True, `state_dict` of\n",
    "            model and optimizer will be cached in memory. Otherwise, they will be saved\n",
    "            to files under the `cache_dir`.\n",
    "        cache_dir (string, optional): path for storing temporary files. If no path is\n",
    "            specified, system-wide temporary directory is used. Notice that this\n",
    "            parameter will be ignored if `memory_cache` is True.\n",
    "    Example:\n",
    "        >>> lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\n",
    "        >>> lr_finder.range_test(dataloader, end_lr=100, num_iter=100)\n",
    "        >>> lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "        >>> lr_finder.reset() # to reset the model and optimizer to their initial state\n",
    "    Reference:\n",
    "    Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n",
    "    fastai/lr_find: https://github.com/fastai/fastai\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device=None,\n",
    "        memory_cache=True,\n",
    "        cache_dir=None,\n",
    "    ):\n",
    "        # Check if the optimizer is already attached to a scheduler\n",
    "        self.optimizer = optimizer\n",
    "        self._check_for_scheduler()\n",
    "\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.history = {\"lr\": [], \"loss\": []}\n",
    "        self.best_loss = None\n",
    "        self.memory_cache = memory_cache\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        # Save the original state of the model and optimizer so they can be restored if\n",
    "        # needed\n",
    "        self.model_device = next(self.model.parameters()).device\n",
    "        self.state_cacher = StateCacher(memory_cache, cache_dir=cache_dir)\n",
    "        self.state_cacher.store(\"model\", self.model.state_dict())\n",
    "        self.state_cacher.store(\"optimizer\", self.optimizer.state_dict())\n",
    "\n",
    "        # If device is None, use the same as the model\n",
    "        if device:\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = self.model_device\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Restores the model and optimizer to their initial states.\"\"\"\n",
    "\n",
    "        self.model.load_state_dict(self.state_cacher.retrieve(\"model\"))\n",
    "        self.optimizer.load_state_dict(self.state_cacher.retrieve(\"optimizer\"))\n",
    "        self.model.to(self.model_device)\n",
    "\n",
    "    def range_test(\n",
    "        self,\n",
    "        train_loader,\n",
    "        val_loader=None,\n",
    "        start_lr=None,\n",
    "        end_lr=10,\n",
    "        num_iter=100,\n",
    "        step_mode=\"exp\",\n",
    "        smooth_f=0.05,\n",
    "        diverge_th=5,\n",
    "        accumulation_steps=1,\n",
    "    ):\n",
    "        \"\"\"Performs the learning rate range test.\n",
    "        Arguments:\n",
    "            train_loader (torch.utils.data.DataLoader): the training set data laoder.\n",
    "            val_loader (torch.utils.data.DataLoader, optional): if `None` the range test\n",
    "                will only use the training loss. When given a data loader, the model is\n",
    "                evaluated after each iteration on that dataset and the evaluation loss\n",
    "                is used. Note that in this mode the test takes significantly longer but\n",
    "                generally produces more precise results. Default: None.\n",
    "            start_lr (float, optional): the starting learning rate for the range test.\n",
    "                Default: None (uses the learning rate from the optimizer).\n",
    "            end_lr (float, optional): the maximum learning rate to test. Default: 10.\n",
    "            num_iter (int, optional): the number of iterations over which the test\n",
    "                occurs. Default: 100.\n",
    "            step_mode (str, optional): one of the available learning rate policies,\n",
    "                linear or exponential (\"linear\", \"exp\"). Default: \"exp\".\n",
    "            smooth_f (float, optional): the loss smoothing factor within the [0, 1[\n",
    "                interval. Disabled if set to 0, otherwise the loss is smoothed using\n",
    "                exponential smoothing. Default: 0.05.\n",
    "            diverge_th (int, optional): the test is stopped when the loss surpasses the\n",
    "                threshold:  diverge_th * best_loss. Default: 5.\n",
    "            accumulation_steps (int, optional): steps for gradient accumulation. If it\n",
    "                is 1, gradients are not accumulated. Default: 1.\n",
    "        Example (fastai approach):\n",
    "            >>> lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\n",
    "            >>> lr_finder.range_test(dataloader, end_lr=100, num_iter=100)\n",
    "        Example (Leslie Smith's approach):\n",
    "            >>> lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\n",
    "            >>> lr_finder.range_test(trainloader, val_loader=val_loader, end_lr=1, num_iter=100, step_mode=\"linear\")\n",
    "        Gradient accumulation is supported; example:\n",
    "            >>> train_data = ...    # prepared dataset\n",
    "            >>> desired_bs, real_bs = 32, 4         # batch size\n",
    "            >>> accumulation_steps = desired_bs // real_bs     # required steps for accumulation\n",
    "            >>> dataloader = torch.utils.data.DataLoader(train_data, batch_size=real_bs, shuffle=True)\n",
    "            >>> acc_lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\n",
    "            >>> acc_lr_finder.range_test(dataloader, end_lr=10, num_iter=100, accumulation_steps=accumulation_steps)\n",
    "        Reference:\n",
    "        [Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed setups](\n",
    "        https://medium.com/huggingface/ec88c3e51255)\n",
    "        [thomwolf/gradient_accumulation](https://gist.github.com/thomwolf/ac7a7da6b1888c2eeac8ac8b9b05d3d3)\n",
    "        \"\"\"\n",
    "\n",
    "        # Reset test results\n",
    "        self.history = {\"lr\": [], \"loss\": []}\n",
    "        self.best_loss = None\n",
    "\n",
    "        # Move the model to the proper device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Check if the optimizer is already attached to a scheduler\n",
    "        self._check_for_scheduler()\n",
    "\n",
    "        # Set the starting learning rate\n",
    "        if start_lr:\n",
    "            self._set_learning_rate(start_lr)\n",
    "\n",
    "        # Initialize the proper learning rate policy\n",
    "        if step_mode.lower() == \"exp\":\n",
    "            lr_schedule = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
    "        elif step_mode.lower() == \"linear\":\n",
    "            lr_schedule = LinearLR(self.optimizer, end_lr, num_iter)\n",
    "        else:\n",
    "            raise ValueError(\"expected one of (exp, linear), got {}\".format(step_mode))\n",
    "\n",
    "        if smooth_f < 0 or smooth_f >= 1:\n",
    "            raise ValueError(\"smooth_f is outside the range [0, 1[\")\n",
    "\n",
    "        # Create an iterator to get data batch by batch\n",
    "        iter_wrapper = DataLoaderIterWrapper(train_loader)\n",
    "        for iteration in tqdm(range(num_iter)):\n",
    "            # Train on batch and retrieve loss\n",
    "            loss = self._train_batch(iter_wrapper, accumulation_steps)\n",
    "            if val_loader:\n",
    "                loss = self._validate(val_loader)\n",
    "\n",
    "            # Update the learning rate\n",
    "            lr_schedule.step()\n",
    "            self.history[\"lr\"].append(lr_schedule.get_lr()[0])\n",
    "\n",
    "            # Track the best loss and smooth it if smooth_f is specified\n",
    "            if iteration == 0:\n",
    "                self.best_loss = loss\n",
    "            else:\n",
    "                if smooth_f > 0:\n",
    "                    loss = smooth_f * loss + (1 - smooth_f) * self.history[\"loss\"][-1]\n",
    "                if loss < self.best_loss:\n",
    "                    self.best_loss = loss\n",
    "\n",
    "            # Check if the loss has diverged; if it has, stop the test\n",
    "            self.history[\"loss\"].append(loss)\n",
    "            if loss > diverge_th * self.best_loss:\n",
    "                print(\"Stopping early, the loss has diverged\")\n",
    "                break\n",
    "\n",
    "        print(\"Learning rate search finished. See the graph with {finder_name}.plot()\")\n",
    "\n",
    "    def _set_learning_rate(self, new_lrs):\n",
    "        if not isinstance(new_lrs, list):\n",
    "            new_lrs = [new_lrs] * len(self.optimizer.param_groups)\n",
    "        if len(new_lrs) != len(self.optimizer.param_groups):\n",
    "            raise ValueError(\n",
    "                \"Length of `new_lrs` is not equal to the number of parameter groups \"\n",
    "                + \"in the given optimizer\"\n",
    "            )\n",
    "\n",
    "        for param_group, new_lr in zip(self.optimizer.param_groups, new_lrs):\n",
    "            param_group[\"lr\"] = new_lr\n",
    "\n",
    "    def _check_for_scheduler(self):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            if \"initial_lr\" in param_group:\n",
    "                raise RuntimeError(\"Optimizer already has a scheduler attached to it\")\n",
    "\n",
    "    def _train_batch(self, iter_wrapper, accumulation_steps):\n",
    "        self.model.train()\n",
    "        total_loss = None  # for late initialization\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        for i in range(accumulation_steps):\n",
    "            \n",
    "            d = next(iter_wrapper)\n",
    "            device = 'cuda'\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            targets_start = d[\"targets_start\"]\n",
    "            targets_end = d[\"targets_end\"]\n",
    "            sentiment = d[\"sentiment\"]\n",
    "            orig_selected = d[\"orig_selected\"]\n",
    "            orig_tweet = d[\"orig_tweet\"]\n",
    "            targets_start = d[\"targets_start\"]\n",
    "            targets_end = d[\"targets_end\"]\n",
    "            offsets = d[\"offsets\"]\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets_start = targets_start.to(device, dtype=torch.long)\n",
    "            targets_end = targets_end.to(device, dtype=torch.long)\n",
    "            \n",
    "            labels = (targets_start, targets_end)\n",
    "            \n",
    "            #inputs, labels = self._move_to_device(inputs, labels)\n",
    "            # Forward pass\n",
    "            outputs = self.model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            loss = self.criterion(outputs, labels)\n",
    "\n",
    "            # Loss should be averaged in each step\n",
    "            loss /= accumulation_steps\n",
    "\n",
    "            # Backward pass\n",
    "            if IS_AMP_AVAILABLE and hasattr(self.optimizer, \"_amp_stash\"):\n",
    "                # For minor performance optimization, see also:\n",
    "                # https://nvidia.github.io/apex/advanced.html#gradient-accumulation-across-iterations\n",
    "                delay_unscale = ((i + 1) % accumulation_steps) != 0\n",
    "\n",
    "                with amp.scale_loss(\n",
    "                    loss, self.optimizer, delay_unscale=delay_unscale\n",
    "                ) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if total_loss is None:\n",
    "                total_loss = loss\n",
    "            else:\n",
    "                total_loss += loss\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return total_loss.item()\n",
    "\n",
    "    def _move_to_device(self, inputs, labels):\n",
    "        def move(obj, device):\n",
    "            if hasattr(obj, \"to\"):\n",
    "                return obj.to(device)\n",
    "            elif isinstance(obj, tuple):\n",
    "                return tuple(move(o, device) for o in obj)\n",
    "            elif isinstance(obj, list):\n",
    "                return [move(o, device) for o in obj]\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: move(o, device) for k, o in obj.items()}\n",
    "            else:\n",
    "                return obj\n",
    "\n",
    "        inputs = move(inputs, self.device)\n",
    "        labels = move(labels, self.device)\n",
    "        return inputs, labels\n",
    "\n",
    "    def _validate(self, dataloader):\n",
    "        # Set model to evaluation mode and disable gradient computation\n",
    "        running_loss = 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for d in dataloader:\n",
    "                \n",
    "                device = 'cuda'\n",
    "                ids = d[\"ids\"]\n",
    "                token_type_ids = d[\"token_type_ids\"]\n",
    "                mask = d[\"mask\"]\n",
    "                sentiment = d[\"sentiment\"]\n",
    "                orig_selected = d[\"orig_selected\"]\n",
    "                orig_tweet = d[\"orig_tweet\"]\n",
    "                targets_start = d[\"targets_start\"]\n",
    "                targets_end = d[\"targets_end\"]\n",
    "                offsets = d[\"offsets\"].numpy()\n",
    "                ids = ids.to(device, dtype=torch.long)\n",
    "                token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "                mask = mask.to(device, dtype=torch.long)\n",
    "                targets_start = targets_start.to(device, dtype=torch.long)\n",
    "                targets_end = targets_end.to(device, dtype=torch.long)\n",
    "            \n",
    "                # Move data to the correct device\n",
    "                #inputs, labels = self._move_to_device(inputs, labels)\n",
    "\n",
    "                if isinstance(inputs, tuple) or isinstance(inputs, list):\n",
    "                    batch_size = inputs[0].size(0)\n",
    "                else:\n",
    "                    batch_size = inputs.size(0)\n",
    "\n",
    "                # Forward pass and loss computation\n",
    "                outputs = self.model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item() * batch_size\n",
    "\n",
    "        return running_loss / len(dataloader.dataset)\n",
    "\n",
    "    def plot(self, skip_start=10, skip_end=5, log_lr=True, show_lr=None, ax=None):\n",
    "        \"\"\"Plots the learning rate range test.\n",
    "        Arguments:\n",
    "            skip_start (int, optional): number of batches to trim from the start.\n",
    "                Default: 10.\n",
    "            skip_end (int, optional): number of batches to trim from the start.\n",
    "                Default: 5.\n",
    "            log_lr (bool, optional): True to plot the learning rate in a logarithmic\n",
    "                scale; otherwise, plotted in a linear scale. Default: True.\n",
    "            show_lr (float, optional): if set, adds a vertical line to visualize the\n",
    "                specified learning rate. Default: None.\n",
    "            ax (matplotlib.axes.Axes, optional): the plot is created in the specified\n",
    "                matplotlib axes object and the figure is not be shown. If `None`, then\n",
    "                the figure and axes object are created in this method and the figure is\n",
    "                shown . Default: None.\n",
    "        Returns:\n",
    "            The matplotlib.axes.Axes object that contains the plot.\n",
    "        \"\"\"\n",
    "\n",
    "        if skip_start < 0:\n",
    "            raise ValueError(\"skip_start cannot be negative\")\n",
    "        if skip_end < 0:\n",
    "            raise ValueError(\"skip_end cannot be negative\")\n",
    "        if show_lr is not None and not isinstance(show_lr, float):\n",
    "            raise ValueError(\"show_lr must be float\")\n",
    "\n",
    "        # Get the data to plot from the history dictionary. Also, handle skip_end=0\n",
    "        # properly so the behaviour is the expected\n",
    "        lrs = self.history[\"lr\"]\n",
    "        losses = self.history[\"loss\"]\n",
    "        if skip_end == 0:\n",
    "            lrs = lrs[skip_start:]\n",
    "            losses = losses[skip_start:]\n",
    "        else:\n",
    "            lrs = lrs[skip_start:-skip_end]\n",
    "            losses = losses[skip_start:-skip_end]\n",
    "\n",
    "        # Create the figure and axes object if axes was not already given\n",
    "        fig = None\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "        # Plot loss as a function of the learning rate\n",
    "        ax.plot(lrs, losses)\n",
    "        if log_lr:\n",
    "            ax.set_xscale(\"log\")\n",
    "        ax.set_xlabel(\"Learning rate\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "\n",
    "        if show_lr is not None:\n",
    "            ax.axvline(x=show_lr, color=\"red\")\n",
    "\n",
    "        # Show only if the figure was created internally\n",
    "        if fig is not None:\n",
    "            plt.show()\n",
    "\n",
    "        return ax\n",
    "\n",
    "\n",
    "class LinearLR(_LRScheduler):\n",
    "    \"\"\"Linearly increases the learning rate between two boundaries over a number of\n",
    "    iterations.\n",
    "    Arguments:\n",
    "        optimizer (torch.optim.Optimizer): wrapped optimizer.\n",
    "        end_lr (float): the final learning rate.\n",
    "        num_iter (int): the number of iterations over which the test occurs.\n",
    "        last_epoch (int, optional): the index of last epoch. Default: -1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
    "        self.end_lr = end_lr\n",
    "        self.num_iter = num_iter\n",
    "        super(LinearLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        curr_iter = self.last_epoch + 1\n",
    "        r = curr_iter / self.num_iter\n",
    "        return [base_lr + r * (self.end_lr - base_lr) for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class ExponentialLR(_LRScheduler):\n",
    "    \"\"\"Exponentially increases the learning rate between two boundaries over a number of\n",
    "    iterations.\n",
    "    Arguments:\n",
    "        optimizer (torch.optim.Optimizer): wrapped optimizer.\n",
    "        end_lr (float): the final learning rate.\n",
    "        num_iter (int): the number of iterations over which the test occurs.\n",
    "        last_epoch (int, optional): the index of last epoch. Default: -1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
    "        self.end_lr = end_lr\n",
    "        self.num_iter = num_iter\n",
    "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        curr_iter = self.last_epoch + 1\n",
    "        r = curr_iter / self.num_iter\n",
    "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class StateCacher(object):\n",
    "    def __init__(self, in_memory, cache_dir=None):\n",
    "        self.in_memory = in_memory\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        if self.cache_dir is None:\n",
    "            import tempfile\n",
    "\n",
    "            self.cache_dir = tempfile.gettempdir()\n",
    "        else:\n",
    "            if not os.path.isdir(self.cache_dir):\n",
    "                raise ValueError(\"Given `cache_dir` is not a valid directory.\")\n",
    "\n",
    "        self.cached = {}\n",
    "\n",
    "    def store(self, key, state_dict):\n",
    "        if self.in_memory:\n",
    "            self.cached.update({key: copy.deepcopy(state_dict)})\n",
    "        else:\n",
    "            fn = os.path.join(self.cache_dir, \"state_{}_{}.pt\".format(key, id(self)))\n",
    "            self.cached.update({key: fn})\n",
    "            torch.save(state_dict, fn)\n",
    "\n",
    "    def retrieve(self, key):\n",
    "        if key not in self.cached:\n",
    "            raise KeyError(\"Target {} was not cached.\".format(key))\n",
    "\n",
    "        if self.in_memory:\n",
    "            return self.cached.get(key)\n",
    "        else:\n",
    "            fn = self.cached.get(key)\n",
    "            if not os.path.exists(fn):\n",
    "                raise RuntimeError(\n",
    "                    \"Failed to load state in {}. File doesn't exist anymore.\".format(fn)\n",
    "                )\n",
    "            state_dict = torch.load(fn, map_location=lambda storage, location: storage)\n",
    "            return state_dict\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Check whether there are unused cached files existing in `cache_dir` before\n",
    "        this instance being destroyed.\"\"\"\n",
    "\n",
    "        if self.in_memory:\n",
    "            return\n",
    "\n",
    "        for k in self.cached:\n",
    "            if os.path.exists(self.cached[k]):\n",
    "                os.remove(self.cached[k])\n",
    "\n",
    "\n",
    "class DataLoaderIterWrapper(object):\n",
    "    \"\"\"A wrapper for iterating `torch.utils.data.DataLoader` with the ability to reset\n",
    "    itself while `StopIteration` is raised.\"\"\"\n",
    "\n",
    "    def __init__(self, data_loader, auto_reset=True):\n",
    "        self.data_loader = data_loader\n",
    "        self.auto_reset = auto_reset\n",
    "        self._iterator = iter(data_loader)\n",
    "\n",
    "    def __next__(self):\n",
    "        # Get a new set of inputs and labels\n",
    "        try:\n",
    "            d = next(self._iterator)\n",
    "        except StopIteration:\n",
    "            if not self.auto_reset:\n",
    "                raise\n",
    "            self._iterator = iter(self.data_loader)\n",
    "            d = next(self._iterator)\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lr():\n",
    "    dfx = pd.read_csv(config.TRAINING_FILE)\n",
    "\n",
    "    fold = 0\n",
    "    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    train_dataset = TweetDataset(\n",
    "        tweet=df_train.text.values,\n",
    "        sentiment=df_train.sentiment.values,\n",
    "        selected_text=df_train.selected_text.values\n",
    "    )\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.TRAIN_BATCH_SIZE,\n",
    "        num_workers=4\n",
    "    )\n",
    "        \n",
    "    device = torch.device(\"cuda\")\n",
    "    model_config = transformers.RobertaConfig.from_pretrained(config.MODEL_NAME)\n",
    "    #model_config.output_hidden_states = True\n",
    "    model = TweetModel(conf=model_config)\n",
    "    model.to(device)\n",
    "    \n",
    "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_parameters, lr=1e-7)\n",
    "\n",
    "    \n",
    "    criterion = loss_fn\n",
    "    \n",
    "    lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "    lr_finder.range_test(train_data_loader, end_lr=0.01, num_iter=1000)\n",
    "    lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "    lr_finder.reset() # to reset the model and optimizer to their initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cbd15f4b564cd994fcf3dcd0c5d236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
