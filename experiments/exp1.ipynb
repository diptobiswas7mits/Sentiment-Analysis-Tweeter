{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.autonotebook import tqdm\n",
    "#import utils\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    MAX_LEN = 192\n",
    "    TRAIN_BATCH_SIZE = 16\n",
    "    VALID_BATCH_SIZE = 8\n",
    "    GRAD_ACC_STEPS = 2\n",
    "    EPOCHS = 3 # 5 was useless, earlystopping kicked in\n",
    "    LEARNING_RATE = 3e-5\n",
    "    DATA_DIR = Path('')\n",
    "    MODEL_NAME = \"roberta-base\"\n",
    "    TRAINING_FILE = \"train_folds.csv\"\n",
    "    TOKENIZER = tokenizers.ByteLevelBPETokenizer( ##explore this\n",
    "        vocab_file=f\"vocab.json\", \n",
    "        merges_file=f\"merges.txt\", \n",
    "        lowercase=True,\n",
    "        add_prefix_space=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class TweetModel(transformers.BertPreTrainedModel):\n",
    "    def __init__(self, conf):\n",
    "        super(TweetModel, self).__init__(conf)\n",
    "        self.roberta = transformers.RobertaModel.from_pretrained(config.MODEL_NAME, config=conf)\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.l0 = nn.Linear(768 * 2, 2)\n",
    "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, _, out = self.roberta( #\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        out = torch.cat((out[-1], out[-2]), dim=-1)\n",
    "        out = self.drop_out(out)\n",
    "        logits = self.l0(out)\n",
    "\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        print(start_logits, end_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
    "#     len_st = len(selected_text)\n",
    "#     idx0 = None\n",
    "#     idx1 = None\n",
    "#     for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n",
    "#         if tweet[ind: ind+len_st] == selected_text:\n",
    "#             idx0 = ind\n",
    "#             idx1 = ind + len_st - 1\n",
    "#             break\n",
    "\n",
    "#     char_targets = [0] * len(tweet)\n",
    "#     if idx0 != None and idx1 != None:\n",
    "#         for ct in range(idx0, idx1 + 1):\n",
    "#             char_targets[ct] = 1\n",
    "    \n",
    "#     tok_tweet = tokenizer.encode(tweet)\n",
    "#     input_ids_orig = tok_tweet.ids[1:-1]\n",
    "#     tweet_offsets = tok_tweet.offsets[1:-1]\n",
    "    \n",
    "#     target_idx = []\n",
    "#     for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
    "#         if sum(char_targets[offset1: offset2]) > 0:\n",
    "#             target_idx.append(j)\n",
    "    \n",
    "#     targets_start = target_idx[0]\n",
    "#     targets_end = target_idx[-1]\n",
    "\n",
    "#     sentiment_id = {\n",
    "#         'positive': 3893,\n",
    "#         'negative': 4997,\n",
    "#         'neutral': 8699\n",
    "#     }\n",
    "    \n",
    "#     input_ids = [101] + [sentiment_id[sentiment]] + [102] + input_ids_orig + [102]\n",
    "#     token_type_ids = [0, 0, 0] + [1] * (len(input_ids_orig) + 1)\n",
    "#     mask = [1] * len(token_type_ids)\n",
    "#     tweet_offsets = [(0, 0)] * 3 + tweet_offsets + [(0, 0)]\n",
    "#     targets_start += 3\n",
    "#     targets_end += 3\n",
    "\n",
    "#     padding_length = max_len - len(input_ids)\n",
    "#     if padding_length > 0:\n",
    "#         input_ids = input_ids + ([0] * padding_length)\n",
    "#         mask = mask + ([0] * padding_length)\n",
    "#         token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "#         tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
    "    \n",
    "#     return {\n",
    "#         'ids': input_ids,\n",
    "#         'mask': mask,\n",
    "#         'token_type_ids': token_type_ids,\n",
    "#         'targets_start': targets_start,\n",
    "#         'targets_end': targets_end,\n",
    "#         'orig_tweet': tweet,\n",
    "#         'orig_selected': selected_text,\n",
    "#         'sentiment': sentiment,\n",
    "#         'offsets': tweet_offsets\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
    "    tweet = \" \" + \" \".join(str(tweet).split())\n",
    "    selected_text = \" \" + \" \".join(str(selected_text).split())\n",
    "\n",
    "    len_st = len(selected_text) - 1\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st - 1\n",
    "            break\n",
    "\n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 != None and idx1 != None:\n",
    "        for ct in range(idx0, idx1 + 1):\n",
    "            char_targets[ct] = 1\n",
    "    \n",
    "    tok_tweet = tokenizer.encode(tweet)\n",
    "    input_ids_orig = tok_tweet.ids\n",
    "    tweet_offsets = tok_tweet.offsets\n",
    "    \n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
    "        if sum(char_targets[offset1: offset2]) > 0:\n",
    "            target_idx.append(j)\n",
    "    \n",
    "    targets_start = target_idx[0]\n",
    "    targets_end = target_idx[-1]\n",
    "\n",
    "    sentiment_id = {\n",
    "        'positive': 1313,\n",
    "        'negative': 2430,\n",
    "        'neutral': 7974\n",
    "    }\n",
    "    \n",
    "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
    "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n",
    "    mask = [1] * len(token_type_ids)\n",
    "    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
    "    targets_start += 4\n",
    "    targets_end += 4\n",
    "\n",
    "    padding_length = max_len - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([1] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
    "    \n",
    "    return {\n",
    "        'ids': input_ids,\n",
    "        'mask': mask,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'targets_start': targets_start,\n",
    "        'targets_end': targets_end,\n",
    "        'orig_tweet': tweet,\n",
    "        'orig_selected': selected_text,\n",
    "        'sentiment': sentiment,\n",
    "        'offsets': tweet_offsets\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset:\n",
    "    def __init__(self, tweet, sentiment, selected_text):\n",
    "        self.tweet = tweet\n",
    "        self.sentiment = sentiment\n",
    "        self.selected_text = selected_text\n",
    "        self.tokenizer = config.TOKENIZER\n",
    "        self.max_len = config.MAX_LEN\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = process_data(\n",
    "            self.tweet[item], \n",
    "            self.selected_text[item], \n",
    "            self.sentiment[item],\n",
    "            self.tokenizer,\n",
    "            self.max_len\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
    "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
    "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
    "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
    "            'orig_tweet': data[\"orig_tweet\"],\n",
    "            'orig_selected': data[\"orig_selected\"],\n",
    "            'sentiment': data[\"sentiment\"],\n",
    "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def calculate_jaccard_score(\n",
    "    original_tweet, \n",
    "    target_string, \n",
    "    sentiment_val, \n",
    "    idx_start, \n",
    "    idx_end, \n",
    "    offsets,\n",
    "    verbose=False):\n",
    "    \n",
    "    if idx_end < idx_start:\n",
    "        idx_end = idx_start\n",
    "    \n",
    "    filtered_output  = \"\"\n",
    "    for ix in range(idx_start, idx_end + 1):\n",
    "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
    "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
    "            filtered_output += \" \"\n",
    "\n",
    "    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n",
    "        filtered_output = original_tweet\n",
    "\n",
    "    jac = jaccard(target_string.strip(), filtered_output.strip())\n",
    "    return jac, filtered_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def calculate_jaccard_score(\n",
    "#     original_tweet, \n",
    "#     target_string, \n",
    "#     sentiment_val, \n",
    "#     idx_start, \n",
    "#     idx_end, \n",
    "#     offsets,\n",
    "#     verbose=False):\n",
    "    \n",
    "#     if idx_end < idx_start:\n",
    "#         idx_end = idx_start\n",
    "    \n",
    "#     filtered_output  = \"\"\n",
    "#     for ix in range(idx_start, idx_end + 1):\n",
    "#         filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
    "#         if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
    "#             filtered_output += \" \"\n",
    "\n",
    "#     if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n",
    "#         filtered_output = original_tweet\n",
    "\n",
    "#     jac = utils.jaccard(target_string.strip(), filtered_output.strip())\n",
    "#     return jac, filtered_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value.\n",
    "\n",
    "    Examples::\n",
    "        >>> # Initialize a meter to record loss\n",
    "        >>> losses = AverageMeter()\n",
    "        >>> # Update meter after every minibatch update\n",
    "        >>> losses.update(loss_value, batch_size)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, model_path):\n",
    "\n",
    "        score = val_loss#-val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, model_path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, model_path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, model_path):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def eval_fn(data_loader, model, device):\n",
    "#     model.eval()\n",
    "#     losses = utils.AverageMeter()\n",
    "#     jaccards = utils.AverageMeter()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "#         for bi, d in enumerate(tk0):\n",
    "#             ids = d[\"ids\"]\n",
    "#             token_type_ids = d[\"token_type_ids\"]\n",
    "#             mask = d[\"mask\"]\n",
    "#             sentiment = d[\"sentiment\"]\n",
    "#             orig_selected = d[\"orig_selected\"]\n",
    "#             orig_tweet = d[\"orig_tweet\"]\n",
    "#             targets_start = d[\"targets_start\"]\n",
    "#             targets_end = d[\"targets_end\"]\n",
    "#             offsets = d[\"offsets\"].numpy()\n",
    "\n",
    "#             ids = ids.to(device, dtype=torch.long)\n",
    "#             token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "#             mask = mask.to(device, dtype=torch.long)\n",
    "#             targets_start = targets_start.to(device, dtype=torch.long)\n",
    "#             targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "#             outputs_start, outputs_end = model(\n",
    "#                 ids=ids,\n",
    "#                 mask=mask,\n",
    "#                 token_type_ids=token_type_ids\n",
    "#             )\n",
    "#             loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
    "#             outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "#             outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "#             jaccard_scores = []\n",
    "#             for px, tweet in enumerate(orig_tweet):\n",
    "#                 selected_tweet = orig_selected[px]\n",
    "#                 tweet_sentiment = sentiment[px]\n",
    "#                 jaccard_score, _ = calculate_jaccard_score(\n",
    "#                     original_tweet=tweet,\n",
    "#                     target_string=selected_tweet,\n",
    "#                     sentiment_val=tweet_sentiment,\n",
    "#                     idx_start=np.argmax(outputs_start[px, :]),\n",
    "#                     idx_end=np.argmax(outputs_end[px, :]),\n",
    "#                     offsets=offsets[px]\n",
    "#                 )\n",
    "#                 jaccard_scores.append(jaccard_score)\n",
    "\n",
    "#             jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "#             losses.update(loss.item(), ids.size(0))\n",
    "#             tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n",
    "    \n",
    "#     print(f\"Jaccard = {jaccards.avg}\")\n",
    "#     return jaccards.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    start_loss = loss_fct(start_logits, start_positions)\n",
    "    end_loss = loss_fct(end_logits, end_positions)\n",
    "    total_loss = (start_loss + end_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    jaccards = AverageMeter()\n",
    "\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    \n",
    "    for bi, d in enumerate(tk0):\n",
    "\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        sentiment = d[\"sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        offsets = d[\"offsets\"]\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets_start = targets_start.to(device, dtype=torch.long)\n",
    "        targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs_start, outputs_end = model(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
    "        loss = loss / config.GRAD_ACC_STEPS\n",
    "        loss.backward()\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "        if (bi+1) % config.GRAD_ACC_STEPS == 0:\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step() \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "        jaccard_scores = []\n",
    "        for px, tweet in enumerate(orig_tweet):\n",
    "            selected_tweet = orig_selected[px]\n",
    "            tweet_sentiment = sentiment[px]\n",
    "            jaccard_score, _ = calculate_jaccard_score(\n",
    "                original_tweet=tweet,\n",
    "                target_string=selected_tweet,\n",
    "                sentiment_val=tweet_sentiment,\n",
    "                idx_start=np.argmax(outputs_start[px, :]),\n",
    "                idx_end=np.argmax(outputs_end[px, :]),\n",
    "                offsets=offsets[px]\n",
    "            )\n",
    "            jaccard_scores.append(jaccard_score)\n",
    "\n",
    "        jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "        losses.update(loss.item(), ids.size(0))\n",
    "        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    jaccards = AverageMeter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "        for bi, d in enumerate(tk0):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            sentiment = d[\"sentiment\"]\n",
    "            orig_selected = d[\"orig_selected\"]\n",
    "            orig_tweet = d[\"orig_tweet\"]\n",
    "            targets_start = d[\"targets_start\"]\n",
    "            targets_end = d[\"targets_end\"]\n",
    "            offsets = d[\"offsets\"].numpy()\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets_start = targets_start.to(device, dtype=torch.long)\n",
    "            targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "            outputs_start, outputs_end = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
    "            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "            jaccard_scores = []\n",
    "            for px, tweet in enumerate(orig_tweet):\n",
    "                selected_tweet = orig_selected[px]\n",
    "                tweet_sentiment = sentiment[px]\n",
    "                jaccard_score, _ = calculate_jaccard_score(\n",
    "                    original_tweet=tweet,\n",
    "                    target_string=selected_tweet,\n",
    "                    sentiment_val=tweet_sentiment,\n",
    "                    idx_start=np.argmax(outputs_start[px, :]),\n",
    "                    idx_end=np.argmax(outputs_end[px, :]),\n",
    "                    offsets=offsets[px]\n",
    "                )\n",
    "                jaccard_scores.append(jaccard_score)\n",
    "\n",
    "            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "            losses.update(loss.item(), ids.size(0))\n",
    "            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n",
    "    \n",
    "    print(f\"Jaccard = {jaccards.avg}\")\n",
    "    return jaccards.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "    dfx = pd.read_csv(config.TRAINING_FILE)\n",
    "\n",
    "    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = TweetDataset(\n",
    "        tweet=df_train.text.values,\n",
    "        sentiment=df_train.sentiment.values,\n",
    "        selected_text=df_train.selected_text.values\n",
    "    )\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.TRAIN_BATCH_SIZE,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_dataset = TweetDataset(\n",
    "        tweet=df_valid.text.values,\n",
    "        sentiment=df_valid.sentiment.values,\n",
    "        selected_text=df_valid.selected_text.values\n",
    "    )\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config.VALID_BATCH_SIZE,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    model_config = transformers.RobertaConfig.from_pretrained(config.MODEL_NAME)\n",
    "    model_config.output_hidden_states = True\n",
    "    model = TweetModel(conf=model_config)\n",
    "    model.to(device)\n",
    "\n",
    "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_parameters, lr=config.LEARNING_RATE)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=0, \n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(patience=2)#, mode=\"max\")\n",
    "    print(f\"Training is Starting for fold={fold}\")\n",
    "    \n",
    "    for epoch in range(config.EPOCHS):# for epoch in range(config.EPOCHS):\n",
    "        train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n",
    "        jaccard = eval_fn(valid_data_loader, model, device)\n",
    "        print(f\"Jaccard Score = {jaccard}\")\n",
    "        es(jaccard, model, model_path=f\"model_{fold}.bin\")\n",
    "        if es.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is Starting for fold=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200217e3e4634396ae13b43ca03ac700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9123,  0.3620,  0.3564,  ...,  0.6429,  0.6386,  0.3273],\n",
      "        [ 0.3949, -0.0938,  0.7475,  ...,  1.5816,  0.5949,  0.6256],\n",
      "        [ 0.4508,  0.1188,  0.3900,  ...,  0.7170,  0.1505,  1.0884],\n",
      "        ...,\n",
      "        [ 0.4142,  0.2713,  0.4085,  ...,  0.5965,  0.7608,  0.4838],\n",
      "        [ 1.1016,  0.5583,  0.1481,  ...,  0.5951,  0.8918,  0.6371],\n",
      "        [ 0.3598,  0.2364,  0.7523,  ...,  0.9263,  1.4485,  0.6210]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([[-0.3769, -1.0391, -0.7663,  ..., -0.6132, -1.0681, -0.0798],\n",
      "        [-0.6863, -0.4711, -0.1056,  ..., -0.6001, -1.2499, -0.8928],\n",
      "        [-0.6768, -0.8577, -0.7265,  ..., -0.7517, -0.9358, -0.9979],\n",
      "        ...,\n",
      "        [-0.6881, -0.8805, -0.7320,  ..., -0.3080, -0.8769, -0.9246],\n",
      "        [ 0.3586, -0.5595, -0.7263,  ..., -0.8362, -0.8003, -0.7976],\n",
      "        [-0.6664, -0.8769, -0.8300,  ..., -0.0274, -0.3073, -0.9599]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 0.3817,  0.3639,  0.3193,  ...,  0.2496,  0.5246,  0.5260],\n",
      "        [-0.0569,  0.1446,  1.0467,  ...,  0.1275,  0.7668,  0.4958],\n",
      "        [ 0.8494,  0.3108,  0.3572,  ...,  0.3629,  0.4718,  0.0410],\n",
      "        ...,\n",
      "        [ 0.9224,  0.4412,  0.6093,  ...,  0.8818,  0.3086,  0.5911],\n",
      "        [ 0.4106, -0.1801,  0.4092,  ...,  0.8962,  0.7177,  0.7759],\n",
      "        [ 1.0486,  0.1412,  0.5275,  ...,  0.3702,  0.5866,  1.7144]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([[-0.6765, -1.0329, -0.6798,  ...,  0.0270, -0.8828, -0.8021],\n",
      "        [-0.8163, -0.7898,  0.3931,  ..., -1.0291, -1.1608, -0.8700],\n",
      "        [ 0.0171, -1.0557, -0.6624,  ..., -0.7065, -0.7685, -1.1279],\n",
      "        ...,\n",
      "        [-0.3542, -0.3112, -0.6140,  ..., -1.0021, -0.8078, -0.8664],\n",
      "        [-0.6954, -0.6339, -0.6869,  ..., -0.9921, -0.8597, -0.9973],\n",
      "        [-0.3215, -0.3375, -0.5724,  ..., -0.7378, -1.1516, -0.6980]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[-0.1335, -0.0179,  0.3288,  ..., -0.2271,  0.1962,  0.5659],\n",
      "        [ 0.3376, -0.0945,  0.3530,  ...,  0.0847, -0.0538,  0.2740],\n",
      "        [ 0.3153,  0.0820,  0.3319,  ...,  0.1632,  0.4466,  0.3852],\n",
      "        ...,\n",
      "        [ 0.9479, -0.1626,  0.5885,  ..., -0.0228,  0.3766,  0.3357],\n",
      "        [ 0.3733, -0.2152,  0.3109,  ...,  0.4536,  0.0688, -0.1863],\n",
      "        [ 0.2739,  0.8279,  0.4497,  ...,  0.5016, -0.0411, -0.1345]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([[-0.8370, -1.1593, -0.6786,  ..., -1.4180, -0.8578, -1.2885],\n",
      "        [-0.7161, -0.8763, -0.7234,  ..., -0.6870, -0.8136, -1.0103],\n",
      "        [-0.6430, -0.8197, -0.7205,  ..., -0.7277, -0.4169, -1.2941],\n",
      "        ...,\n",
      "        [-0.4962, -1.2585, -1.0579,  ..., -1.1172, -0.6531, -0.7035],\n",
      "        [-0.7454, -1.0584, -1.1776,  ..., -1.5419, -1.0588, -1.1980],\n",
      "        [-0.6241, -0.3953, -1.0813,  ..., -0.9705, -0.9281, -1.1949]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 3.7757e-01,  1.0050e-01,  3.2740e-01,  ...,  5.1877e-01,\n",
      "          4.0485e-01,  1.0810e+00],\n",
      "        [ 3.8452e-01, -7.3425e-06,  2.1953e-01,  ..., -8.4865e-02,\n",
      "          3.6515e-01,  3.5489e-01],\n",
      "        [ 3.4382e-01, -9.7070e-02,  3.8323e-01,  ...,  6.9151e-02,\n",
      "         -4.3606e-01, -2.4528e-01],\n",
      "        ...,\n",
      "        [-9.2931e-02,  3.1697e-01,  5.7295e-01,  ...,  4.3536e-01,\n",
      "          2.1367e-02,  3.3568e-01],\n",
      "        [ 3.4209e-01, -2.6866e-01,  1.2381e+00,  ...,  1.3151e-01,\n",
      "          5.5987e-01,  4.0694e-01],\n",
      "        [ 3.2320e-01, -1.9472e-01,  7.8409e-01,  ..., -7.9789e-03,\n",
      "          2.7033e-01,  5.8740e-01]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([[-0.7500, -1.1803, -0.7223,  ..., -0.5971, -0.6832, -0.7830],\n",
      "        [-0.6553, -1.1698, -1.0065,  ..., -0.8044, -1.1876, -0.7067],\n",
      "        [-0.6898, -0.9681, -0.8593,  ..., -1.0342, -0.9344, -1.0594],\n",
      "        ...,\n",
      "        [-0.8539, -0.2650, -0.6723,  ..., -1.1167, -0.8695, -0.8996],\n",
      "        [-0.7404, -1.2594, -0.5739,  ..., -1.0427, -1.2353, -1.1945],\n",
      "        [-0.6452, -1.0280, -0.0502,  ..., -1.4467, -1.1794, -0.9353]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 0.5632, -0.2329,  0.3237,  ...,  0.8668,  0.4374, -0.0868],\n",
      "        [ 0.2933, -0.0942,  0.2894,  ...,  0.3044, -0.7945,  0.0595],\n",
      "        [ 0.2846, -0.3036,  0.3065,  ..., -0.3559, -0.3351,  0.1335],\n",
      "        ...,\n",
      "        [-0.1426, -0.3445,  0.3211,  ..., -0.1029, -0.4616, -0.2073],\n",
      "        [ 0.2937,  0.3998,  0.4048,  ..., -0.3104, -0.2938,  0.1067],\n",
      "        [-0.3553, -0.0427,  0.7858,  ..., -0.1277,  0.0216, -0.3166]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([[-0.3430, -1.6038, -0.7393,  ..., -0.4964,  0.1222, -0.8397],\n",
      "        [-0.7136, -1.2070, -0.6962,  ..., -0.2724, -1.1832, -1.3566],\n",
      "        [-0.6663, -1.3113, -0.7099,  ..., -1.4848, -1.2828, -0.9232],\n",
      "        ...,\n",
      "        [-0.8603, -1.2366, -0.6862,  ..., -1.0430, -1.5362, -1.3993],\n",
      "        [-0.7001, -1.1472, -0.4990,  ..., -1.6171, -1.2608, -1.2751],\n",
      "        [-0.8568, -1.3754,  0.0160,  ..., -1.2767, -0.4935, -1.3501]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 0.9624, -0.3797,  0.3193,  ..., -0.4415, -0.0326, -0.4713],\n",
      "        [ 0.2876, -0.1318,  0.4397,  ...,  0.1698, -0.1794, -0.1123],\n",
      "        [-0.1119, -0.4634,  0.0088,  ..., -0.0319, -0.4871, -0.3083],\n",
      "        ...,\n",
      "        [-0.0985, -0.0250,  0.1149,  ...,  0.2539, -0.0230,  0.3191],\n",
      "        [ 0.3156, -0.1187,  0.3260,  ...,  0.2568,  0.0784, -0.3605],\n",
      "        [ 0.3176,  0.0619,  0.1500,  ..., -0.1230,  0.2648, -0.1995]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([[-0.4601, -1.3208, -0.6882,  ..., -1.5041, -0.4986, -1.5856],\n",
      "        [-0.7229, -1.2147, -1.0280,  ..., -1.2072, -1.1943, -1.1538],\n",
      "        [-0.9102, -1.1419, -1.0553,  ..., -0.9553, -1.4213, -1.1387],\n",
      "        ...,\n",
      "        [-0.9186, -1.1941, -0.7471,  ..., -0.5123, -1.3810, -0.7791],\n",
      "        [-0.7221, -1.1300, -0.7617,  ..., -1.1829, -1.0137, -1.6700],\n",
      "        [-0.7133, -1.5468, -1.1356,  ..., -1.4509, -0.9774, -1.5854]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 0.2516, -0.5681,  0.1786,  ..., -0.8035, -0.4355, -0.5332],\n",
      "        [ 0.4982, -0.6374,  0.2009,  ..., -0.6985, -0.6287, -0.7799],\n",
      "        [ 0.3003, -0.7643,  0.7605,  ...,  0.0251, -0.5828,  0.0405],\n",
      "        ...,\n",
      "        [ 0.4699, -0.4755,  0.3442,  ..., -0.5791, -0.2734, -0.6159],\n",
      "        [-0.3428, -0.4386, -0.0141,  ..., -0.1869, -0.2712, -0.6579],\n",
      "        [ 0.3034, -0.2123, -0.7651,  ..., -0.6640, -0.4452, -0.4411]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([[-0.7066, -1.5180, -1.1171,  ..., -1.9910, -1.6010, -1.3478],\n",
      "        [ 0.1390, -1.1806, -1.3429,  ..., -1.2503, -1.2662, -1.7251],\n",
      "        [-0.6743, -1.7094, -0.0069,  ..., -1.5594, -1.9018, -1.6845],\n",
      "        ...,\n",
      "        [ 0.1269, -0.7661, -1.0026,  ..., -1.2152, -1.8838, -1.6993],\n",
      "        [-0.9446, -1.5732, -1.1450,  ..., -0.9504, -1.5911, -1.5965],\n",
      "        [ 0.0502, -1.3154, -1.2807,  ..., -1.1453, -1.2306, -1.0435]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 0.2637, -0.3995,  0.2794,  ..., -0.2591, -0.1269,  0.2606],\n",
      "        [-0.1417, -0.9117,  0.2956,  ...,  0.3457, -0.6831, -0.0452],\n",
      "        [ 0.2388, -0.3488,  0.3486,  ..., -0.3425, -0.1678,  0.9210],\n",
      "        ...,\n",
      "        [ 0.2777, -0.5764,  0.0513,  ..., -0.1219, -0.8318, -0.7724],\n",
      "        [ 0.2777,  0.1571,  0.8385,  ...,  0.1092, -0.1099, -0.3489],\n",
      "        [ 0.2940, -0.5118,  0.2346,  ..., -0.7386, -1.0893, -0.3775]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([[-0.6979, -0.7125, -0.6634,  ..., -1.4085, -0.8259, -0.7163],\n",
      "        [-0.8489, -1.1627, -0.7218,  ..., -1.6389, -1.6591, -1.5307],\n",
      "        [-0.6840, -1.1833,  0.1329,  ..., -1.9255, -1.6593,  0.3691],\n",
      "        ...,\n",
      "        [-0.6948, -1.5661, -1.2950,  ..., -0.8867, -1.8160, -1.8981],\n",
      "        [-0.6776, -1.5529, -0.3945,  ..., -0.8027, -1.0363, -1.7193],\n",
      "        [-0.7301, -0.8504, -1.1144,  ..., -1.4906, -1.6190, -1.3372]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[-0.1933, -0.5342, -0.2798,  ..., -0.8833, -0.9278, -0.9847],\n",
      "        [ 0.3083, -0.4756, -0.1696,  ..., -0.4327, -0.7547, -1.0601],\n",
      "        [ 0.6334, -0.7617, -0.4762,  ..., -0.8192, -0.7443, -0.9887],\n",
      "        ...,\n",
      "        [ 0.2548, -0.9740,  0.4753,  ..., -1.1588,  0.2059, -1.2407],\n",
      "        [ 0.2458, -0.7877, -0.0996,  ..., -1.1231, -0.8551, -0.6295],\n",
      "        [ 0.4386, -0.7856,  0.5814,  ..., -0.7346, -0.8862, -0.8722]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([[-0.8875, -1.6633, -1.2715,  ..., -1.8409, -1.9118, -1.8250],\n",
      "        [-0.7142, -0.7958, -1.4260,  ..., -1.2176, -1.9705, -1.9614],\n",
      "        [-0.3148, -1.6085, -1.1288,  ..., -1.6731, -1.5407, -1.5908],\n",
      "        ...,\n",
      "        [-0.7156, -1.6891,  0.1410,  ..., -1.1788, -0.6519, -1.5907],\n",
      "        [-0.3565, -1.4907, -1.2155,  ..., -1.4381, -1.6006, -0.9691],\n",
      "        [ 0.1115, -1.6474, -1.5174,  ..., -1.2471, -1.7103, -1.2490]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7892, -0.5633,  0.7600,  ..., -0.8201,  0.1220,  0.2437],\n",
      "        [ 0.7412, -0.6438,  0.7737,  ..., -0.1996,  0.3107, -0.7660],\n",
      "        [ 0.2263, -0.8166,  0.3678,  ..., -0.9735, -0.7652, -0.6485],\n",
      "        ...,\n",
      "        [ 0.2433, -0.8660, -0.1575,  ..., -1.0281, -0.9320, -0.9581],\n",
      "        [ 0.2314, -0.7518,  0.2647,  ..., -0.6633,  0.3925, -0.0021],\n",
      "        [ 0.7043, -0.3498,  0.3489,  ..., -0.8499, -0.7463, -1.1032]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([[-0.4189, -1.8359, -0.4370,  ..., -1.9275, -0.8509, -0.7126],\n",
      "        [-0.4801, -1.8592, -0.4188,  ..., -1.4136, -0.1268, -1.8519],\n",
      "        [-0.6901, -1.5863,  0.1303,  ..., -1.6298, -1.6971, -2.0054],\n",
      "        ...,\n",
      "        [-0.6855, -1.5683, -0.8781,  ..., -1.4475, -2.0301, -1.6310],\n",
      "        [-0.7117, -1.4809, -0.7037,  ..., -1.2844,  0.1156, -1.0863],\n",
      "        [ 0.2625, -1.2143, -0.7221,  ..., -1.6655, -2.0238, -1.5865]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 0.2972, -1.1798, -0.2829,  ..., -1.2777, -1.6188, -1.9018],\n",
      "        [ 0.3022, -1.0076,  0.0231,  ..., -1.1419, -1.0908, -1.2840],\n",
      "        [ 0.2305, -1.0691, -0.0214,  ..., -1.5141, -0.8333, -1.0667],\n",
      "        ...,\n",
      "        [ 0.2270, -0.7988, -0.0142,  ..., -1.4639, -1.2974, -1.4098],\n",
      "        [ 0.2395, -0.8327, -0.3981,  ..., -1.6583, -1.3781, -1.2836],\n",
      "        [ 0.2436, -0.6042, -0.2112,  ..., -1.4231, -1.1978, -1.6932]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([[-0.6747, -2.0146, -1.3885,  ..., -1.5329, -2.5476, -2.6093],\n",
      "        [-0.6619, -1.6832, -1.3020,  ..., -1.1297, -1.3894, -1.9087],\n",
      "        [-0.7269, -1.8287, -1.4457,  ..., -2.2218, -2.0202, -1.9279],\n",
      "        ...,\n",
      "        [-0.6906, -1.8533, -1.4489,  ..., -2.2269, -2.2495, -1.8003],\n",
      "        [-0.5961, -1.8930, -1.5113,  ..., -2.0424, -2.1146, -1.8208],\n",
      "        [-0.6911, -1.6910, -0.8719,  ..., -2.2345, -2.4842, -2.2957]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 0.2401, -0.8615, -0.2306,  ..., -1.2574, -0.8596, -0.4885],\n",
      "        [ 0.2191, -0.9681, -0.1651,  ..., -0.2547, -1.1496,  0.3792],\n",
      "        [ 0.8025, -0.5716,  0.6105,  ..., -0.2044, -0.6249, -1.4175],\n",
      "        ...,\n",
      "        [ 0.1124, -1.2761, -0.2309,  ..., -1.3957, -1.4166, -1.5143],\n",
      "        [ 0.3126, -1.2271,  0.2674,  ..., -1.1646, -0.5796, -1.2111],\n",
      "        [-0.1819, -1.0890,  0.7511,  ..., -0.5349,  0.7294, -0.6638]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([[-0.7124, -2.0509, -0.8301,  ..., -1.8517, -1.9570, -1.6817],\n",
      "        [-0.6263, -2.1266, -1.3325,  ..., -1.3507, -2.0665,  0.2086],\n",
      "        [-0.4362, -2.0172, -0.6060,  ..., -1.5194, -1.9852, -1.4076],\n",
      "        ...,\n",
      "        [-0.4460, -1.9829, -1.5674,  ..., -2.0070, -2.2370, -2.2751],\n",
      "        [-0.7021, -2.1560, -0.6690,  ..., -2.4612, -2.3499, -2.2728],\n",
      "        [-0.8196, -1.7380, -0.1883,  ..., -1.5578, -0.4480, -1.7398]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 0.1784, -1.2476, -0.0591,  ..., -2.0761, -0.9169, -1.4822],\n",
      "        [-0.2699, -1.1000, -0.3498,  ..., -1.9272, -1.4477, -0.9782],\n",
      "        [ 0.2464, -1.1875, -0.5629,  ..., -1.2173, -1.7442, -1.8137],\n",
      "        ...,\n",
      "        [ 0.6807, -0.9975, -0.2731,  ..., -1.1877, -1.3723, -1.8697],\n",
      "        [ 0.1731, -1.1170,  0.1720,  ..., -1.6000, -1.7020, -1.6148],\n",
      "        [ 0.2334, -0.8302, -0.5488,  ..., -1.6203, -1.6394, -1.7137]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([[-0.7160, -2.4629, -0.7022,  ..., -2.8221, -2.0671, -2.1403],\n",
      "        [-0.8833, -1.6949, -1.3394,  ..., -2.0482, -2.0793, -2.0490],\n",
      "        [-0.6465, -1.6914, -1.4934,  ..., -2.3963, -2.1031, -2.7237],\n",
      "        ...,\n",
      "        [-0.4100, -1.8944, -1.0536,  ..., -2.2689, -2.2798, -2.2832],\n",
      "        [-0.7250, -1.9432, -0.6765,  ..., -1.6817, -1.7840, -1.9207],\n",
      "        [-0.4612, -1.8014, -0.6808,  ..., -2.1915, -2.3402, -2.2975]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 1.8485e-01, -1.7249e+00,  2.7572e-01,  ..., -1.2273e+00,\n",
      "         -1.9092e+00, -1.9846e+00],\n",
      "        [ 2.9175e-01, -8.3072e-01,  2.3582e-01,  ..., -1.8194e+00,\n",
      "         -1.5269e+00, -9.0495e-01],\n",
      "        [ 2.1962e-01, -8.4301e-01, -4.3372e-01,  ..., -1.3919e+00,\n",
      "         -1.7324e+00, -1.9601e+00],\n",
      "        ...,\n",
      "        [ 2.3721e-01, -6.5741e-01, -3.6441e-04,  ..., -6.8656e-01,\n",
      "         -1.9666e+00, -1.7027e+00],\n",
      "        [ 2.3501e-01, -1.1102e+00,  6.4297e-02,  ..., -1.9519e+00,\n",
      "         -9.0130e-01, -1.9807e+00],\n",
      "        [ 3.2641e-01, -1.0275e+00, -6.0852e-02,  ..., -2.2739e+00,\n",
      "         -1.1669e+00, -1.7570e+00]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([[-0.7188, -2.3135, -0.6834,  ..., -1.4501, -2.2021, -2.2233],\n",
      "        [-0.2379, -1.8294, -0.6549,  ..., -1.9724, -1.5773, -1.9916],\n",
      "        [-0.7047, -1.4258, -0.8018,  ..., -2.1474, -2.0728, -2.1388],\n",
      "        ...,\n",
      "        [-0.7080, -1.8153, -0.3988,  ..., -1.7523, -2.2199, -1.9806],\n",
      "        [-0.6563, -1.7267, -0.8847,  ..., -2.1412, -1.7023, -1.8462],\n",
      "        [-0.6473, -2.1217, -1.0231,  ..., -2.8523, -1.8824, -1.4356]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 0.2120, -2.0609, -0.5513,  ..., -2.4352, -1.7173, -2.6349],\n",
      "        [ 0.6890, -0.6559,  0.0077,  ..., -2.0128, -1.7156, -2.0492],\n",
      "        [ 0.2311, -0.6418, -0.1808,  ..., -1.9636, -2.1667, -1.3917],\n",
      "        ...,\n",
      "        [ 0.2190, -1.4136, -0.6044,  ..., -1.9153, -2.1589, -2.4941],\n",
      "        [ 0.6746, -1.1601,  0.2141,  ..., -2.1622, -2.1481, -1.7755],\n",
      "        [ 0.3578, -0.1224,  0.4145,  ..., -1.9833, -1.8790, -1.7593]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([[-0.6699, -2.7349, -1.7190,  ..., -2.2234, -2.7779, -3.0343],\n",
      "        [-0.2702, -2.2614, -1.2792,  ..., -2.5071, -1.9481, -2.2442],\n",
      "        [-0.6658, -1.6579, -1.2257,  ..., -2.3211, -2.2841, -1.8112],\n",
      "        ...,\n",
      "        [-0.6545, -2.2513, -1.4619,  ..., -2.7996, -2.2366, -2.5551],\n",
      "        [-0.3775, -1.7392, -0.6431,  ..., -2.6255, -2.2264, -2.4606],\n",
      "        [ 0.1196, -1.1479, -0.5961,  ..., -2.2620, -2.5822, -2.6388]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 0.1790, -0.6066,  0.2264,  ..., -1.4991, -1.9259, -1.8781],\n",
      "        [ 0.2718, -1.2181, -0.0878,  ..., -1.6737, -1.9061, -1.8253],\n",
      "        [ 0.2119, -1.3715,  0.4480,  ..., -1.4235, -0.7730, -1.9320],\n",
      "        ...,\n",
      "        [ 0.4449, -1.6772,  0.2793,  ..., -2.3834, -2.2074, -2.4693],\n",
      "        [ 0.2308, -1.3354, -0.4407,  ..., -2.0824, -1.8397, -1.6907],\n",
      "        [ 0.7934, -1.0704, -0.4006,  ..., -2.0091, -1.6392, -2.1302]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([[-0.6083, -0.6243, -0.5967,  ..., -1.8778, -2.0517, -2.0165],\n",
      "        [-0.6039, -1.5808, -0.5373,  ..., -2.6368, -2.4786, -1.8337],\n",
      "        [-0.6879, -2.0177, -1.3984,  ..., -2.6218, -2.1422, -2.3860],\n",
      "        ...,\n",
      "        [ 0.1993, -2.0407, -0.5946,  ..., -2.5091, -2.3890, -2.0225],\n",
      "        [-0.6433, -2.1764, -0.7128,  ..., -1.9229, -2.3533, -2.3606],\n",
      "        [-0.4092, -1.8146, -1.1654,  ..., -2.5722, -2.6663, -2.4482]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 0.1944, -0.9125,  0.4327,  ..., -2.2548, -2.6226, -2.3431],\n",
      "        [ 0.1497, -0.7821,  0.2730,  ..., -2.1314, -2.0443, -3.0175],\n",
      "        [ 0.2106, -1.1926, -0.7684,  ..., -2.9761, -2.4697, -2.6926],\n",
      "        ...,\n",
      "        [ 0.1935, -2.2753,  0.2073,  ..., -2.2589, -2.4200, -2.2207],\n",
      "        [ 0.2439, -1.8357, -0.1194,  ..., -2.3545, -3.0524, -2.7373],\n",
      "        [ 0.1569, -1.9033, -0.0486,  ..., -2.7850, -2.4748, -2.7288]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([[-0.6048, -1.9523, -0.3583,  ..., -2.5980, -2.2783, -2.8650],\n",
      "        [-0.5840, -2.4592, -0.9090,  ..., -2.2284, -3.0327, -2.7848],\n",
      "        [-0.6666, -1.0844, -0.9374,  ..., -1.7633, -2.0522, -2.6783],\n",
      "        ...,\n",
      "        [-0.6520, -2.1701, -0.6219,  ..., -2.8438, -2.3188, -2.7549],\n",
      "        [-0.6694, -1.8127,  0.1754,  ..., -2.3843, -2.8101, -2.6622],\n",
      "        [-0.7009, -1.6130, -0.4653,  ..., -2.6319, -2.4216, -3.0413]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 0.1728, -2.0425, -0.1233,  ..., -2.4264, -2.4294, -2.1347],\n",
      "        [ 0.3701, -1.1853,  0.1739,  ..., -1.4944, -2.1412, -2.0375],\n",
      "        [ 0.1969, -1.4308,  0.5237,  ..., -2.3019, -2.4678, -2.4091],\n",
      "        ...,\n",
      "        [ 0.2164, -1.9515, -0.5960,  ..., -2.0457, -2.3185, -2.5276],\n",
      "        [-0.4860, -1.4513, -0.1388,  ..., -2.2110, -2.7327, -2.9311],\n",
      "        [ 0.2019, -1.2536, -0.0602,  ..., -3.5123, -1.9269, -2.2241]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([[-0.6469, -2.0825, -1.1011,  ..., -2.5963, -2.8382, -2.5009],\n",
      "        [ 0.1550, -1.5733, -0.6245,  ..., -1.9880, -2.3232, -2.1098],\n",
      "        [-0.6219, -1.3332,  0.0384,  ..., -1.9322, -2.0798, -1.9103],\n",
      "        ...,\n",
      "        [-0.4909, -1.5172, -0.6334,  ..., -1.8219, -2.3370, -2.5313],\n",
      "        [-0.6097, -3.1067,  0.4792,  ..., -3.0383, -2.5535, -2.9552],\n",
      "        [-0.6761, -1.4291, -0.9187,  ..., -2.8936, -2.8207, -2.4240]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2196, -1.6787, -0.4628,  ..., -2.3599, -2.5365, -2.0093],\n",
      "        [ 0.1951, -0.2529, -0.6666,  ..., -3.3556, -2.4381, -3.1400],\n",
      "        [ 0.2677, -0.7990, -0.0543,  ..., -2.3450, -2.7388, -2.3834],\n",
      "        ...,\n",
      "        [ 0.6634, -1.5819, -0.6440,  ..., -2.9575, -2.3824, -2.8615],\n",
      "        [ 0.1836, -0.3658, -0.3641,  ..., -2.5040, -2.2966, -2.2764],\n",
      "        [ 0.2164, -1.0264,  0.4614,  ..., -2.0197, -2.9789, -3.0099]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([[-0.6441, -2.4800, -0.8318,  ..., -3.2458, -2.8876, -2.4092],\n",
      "        [-0.6398, -0.8399,  0.5344,  ..., -2.3243, -2.2758, -2.9330],\n",
      "        [-0.5882, -1.9343, -0.0409,  ..., -2.5066, -3.0426, -3.1130],\n",
      "        ...,\n",
      "        [-0.3465, -2.7478, -1.3951,  ..., -2.9955, -3.0068, -2.9388],\n",
      "        [-0.6034, -1.1722, -0.7110,  ..., -2.4898, -2.2870, -1.8141],\n",
      "        [-0.5997, -1.8256,  0.4251,  ..., -2.9264, -2.9907, -2.6995]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[-0.5440, -1.6453, -0.6546,  ..., -2.4238, -1.8718, -2.0578],\n",
      "        [ 0.1433, -0.8330,  0.7816,  ..., -2.7317, -1.6049, -1.8429],\n",
      "        [ 0.3456, -1.3660, -0.7133,  ..., -2.8568, -2.5910, -2.4881],\n",
      "        ...,\n",
      "        [ 0.2423, -1.2173, -0.9672,  ..., -2.8844, -2.8743, -2.9785],\n",
      "        [ 0.1850, -1.2872,  0.3084,  ..., -3.1258, -3.1635, -2.6832],\n",
      "        [ 0.2267, -1.5493, -0.1010,  ..., -2.9492, -2.9659, -2.2340]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([[-0.7405, -2.5120, -1.1123,  ..., -3.0151, -2.8877, -2.7447],\n",
      "        [-0.6269, -1.5828,  0.8518,  ..., -3.0264, -1.6611, -2.3210],\n",
      "        [ 0.1686, -2.2484, -1.3128,  ..., -2.4056, -3.0274, -3.0268],\n",
      "        ...,\n",
      "        [-0.6690, -2.3372, -1.0888,  ..., -3.0817, -2.9290, -2.9480],\n",
      "        [-0.6363, -1.9567,  0.1551,  ..., -2.7748, -2.8166, -2.5963],\n",
      "        [-0.6179, -2.2935,  0.1729,  ..., -2.6058, -3.0050, -2.5897]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 0.2062,  0.3699, -0.5152,  ..., -2.7556, -2.4385, -2.6507],\n",
      "        [ 0.6570, -0.8424, -0.4026,  ..., -3.0307, -3.0031, -1.9055],\n",
      "        [ 0.1766, -1.9449, -0.2346,  ..., -3.0248, -3.2070, -2.8996],\n",
      "        ...,\n",
      "        [ 0.2122, -1.1459, -0.1386,  ..., -1.9578, -2.5058, -2.5044],\n",
      "        [ 0.1848, -0.6185,  0.6310,  ..., -2.1061, -2.6905, -2.9488],\n",
      "        [ 0.2202, -1.0684, -0.9014,  ..., -2.9121, -2.6366, -3.2937]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([[-0.6433, -1.0440, -0.6742,  ..., -2.5050, -2.8783, -3.0237],\n",
      "        [-0.1450, -1.8786,  0.4092,  ..., -2.8649, -2.9755, -2.2779],\n",
      "        [-0.6125, -2.3897, -0.6590,  ..., -3.6485, -3.0355, -2.8298],\n",
      "        ...,\n",
      "        [-0.5880, -1.1426,  0.4115,  ..., -2.7013, -2.6495, -2.2549],\n",
      "        [-0.6260, -0.8443,  0.5415,  ..., -3.0416, -2.6555, -2.3129],\n",
      "        [-0.6237, -2.3413,  0.1257,  ..., -3.5071, -3.2582, -3.1167]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 0.1611, -0.8223, -0.1816,  ..., -3.2328, -3.2063, -3.3042],\n",
      "        [ 0.0735, -1.2609, -1.0524,  ..., -2.7467, -2.8950, -3.3306],\n",
      "        [ 0.1562, -1.8777, -1.0722,  ..., -2.4006, -1.9945, -3.6255],\n",
      "        ...,\n",
      "        [ 0.1092, -1.5577,  0.9549,  ..., -1.7969, -3.0823, -2.5598],\n",
      "        [ 0.1030, -1.0734,  1.2115,  ..., -3.3563, -2.3860, -1.8966],\n",
      "        [ 0.2423, -1.5389,  0.0586,  ..., -2.6846, -2.3126, -2.5331]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([[-0.5820, -2.3977, -0.3130,  ..., -3.0871, -2.8308, -2.9993],\n",
      "        [-0.6237, -2.1350, -1.5104,  ..., -3.2963, -3.1748, -2.9305],\n",
      "        [-0.6441, -2.2814, -2.3779,  ..., -3.2784, -3.2316, -3.0674],\n",
      "        ...,\n",
      "        [-0.6037, -1.6742,  0.1832,  ..., -1.9147, -2.6622, -2.8708],\n",
      "        [-0.3982, -1.6286,  0.6260,  ..., -2.1473, -3.0135, -2.2091],\n",
      "        [-0.5804, -1.9754, -0.6997,  ..., -2.5561, -2.3159, -2.5173]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d64cf0f07b3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-891449b5cc5c>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(fold)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m# for epoch in range(config.EPOCHS):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mjaccard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Jaccard Score = {jaccard}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-c0b5a6cb4d5c>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(data_loader, model, optimizer, device, scheduler)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0moutputs_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0moutputs_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mjaccard_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run(fold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(fold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(fold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(fold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(fold=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_test.loc[:, \"selected_text\"] = df_test.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_config = transformers.RobertaConfig.from_pretrained(config.MODEL_NAME)\n",
    "model_config.output_hidden_states = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = TweetModel(conf=model_config)\n",
    "model1.to(device)\n",
    "model1.load_state_dict(torch.load(\"model_0.bin\"))\n",
    "model1.eval()\n",
    "\n",
    "model2 = TweetModel(conf=model_config)\n",
    "model2.to(device)\n",
    "model2.load_state_dict(torch.load(\"model_1.bin\"))\n",
    "model2.eval()\n",
    "\n",
    "model3 = TweetModel(conf=model_config)\n",
    "model3.to(device)\n",
    "model3.load_state_dict(torch.load(\"model_2.bin\"))\n",
    "model3.eval()\n",
    "\n",
    "model4 = TweetModel(conf=model_config)\n",
    "model4.to(device)\n",
    "model4.load_state_dict(torch.load(\"model_3.bin\"))\n",
    "model4.eval()\n",
    "\n",
    "model5 = TweetModel(conf=model_config)\n",
    "model5.to(device)\n",
    "model5.load_state_dict(torch.load(\"model_4.bin\"))\n",
    "model5.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TweetDataset(\n",
    "        tweet=df_test.text.values,\n",
    "        sentiment=df_test.sentiment.values,\n",
    "        selected_text=df_test.selected_text.values\n",
    "    )\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=config.VALID_BATCH_SIZE,\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    for bi, d in enumerate(tk0):\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        sentiment = d[\"sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        offsets = d[\"offsets\"].numpy()\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets_start = targets_start.to(device, dtype=torch.long)\n",
    "        targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "#         outputs_start1, outputs_end1 = model1(\n",
    "#             ids=ids,\n",
    "#             mask=mask,\n",
    "#             token_type_ids=token_type_ids\n",
    "#         )\n",
    "        \n",
    "        outputs_start2, outputs_end2 = model2(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "#         outputs_start3, outputs_end3 = model3(\n",
    "#             ids=ids,\n",
    "#             mask=mask,\n",
    "#             token_type_ids=token_type_ids\n",
    "#         )\n",
    "        \n",
    "#         outputs_start4, outputs_end4 = model4(\n",
    "#             ids=ids,\n",
    "#             mask=mask,\n",
    "#             token_type_ids=token_type_ids\n",
    "#         )\n",
    "        \n",
    "#         outputs_start5, outputs_end5 = model5(\n",
    "#             ids=ids,\n",
    "#             mask=mask,\n",
    "#             token_type_ids=token_type_ids\n",
    "#         )\n",
    "#         outputs_start = (outputs_start1 + outputs_start2 + outputs_start3 + outputs_start4 + outputs_start5) / 5\n",
    "#         outputs_end = (outputs_end1 + outputs_end2 + outputs_end3 + outputs_end4 + outputs_end5) / 5\n",
    "        \n",
    "        outputs_start = outputs_start2\n",
    "        outputs_end = outputs_end2\n",
    "        \n",
    "        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "        jaccard_scores = []\n",
    "        for px, tweet in enumerate(orig_tweet):\n",
    "            selected_tweet = orig_selected[px]\n",
    "            tweet_sentiment = sentiment[px]\n",
    "            _, output_sentence = calculate_jaccard_score(\n",
    "                original_tweet=tweet,\n",
    "                target_string=selected_tweet,\n",
    "                sentiment_val=tweet_sentiment,\n",
    "                idx_start=np.argmax(outputs_start[px, :]),\n",
    "                idx_end=np.argmax(outputs_end[px, :]),\n",
    "                offsets=offsets[px]\n",
    "            )\n",
    "            final_output.append(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(\"sample_submission.csv\")\n",
    "sample.loc[:, 'selected_text'] = final_output\n",
    "sample.to_csv(\"submission-fold2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
