{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.autonotebook import tqdm\n",
    "#import utils\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    MAX_LEN = 192\n",
    "    TRAIN_BATCH_SIZE = 16\n",
    "    VALID_BATCH_SIZE = 8\n",
    "    #GRAD_ACC_STEPS = 1\n",
    "    EPOCHS = 3 # 5 was useless, earlystopping kicked in\n",
    "    LEARNING_RATE = 3e-5\n",
    "    DATA_DIR = Path('')\n",
    "    MODEL_NAME = \"roberta-base\"\n",
    "    TRAINING_FILE = \"train_folds_v2.csv\"\n",
    "    TOKENIZER = tokenizers.ByteLevelBPETokenizer( ##explore this\n",
    "        vocab_file=f\"vocab.json\", \n",
    "        merges_file=f\"merges.txt\", \n",
    "        lowercase=True,\n",
    "        add_prefix_space=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetModel(transformers.BertPreTrainedModel):\n",
    "    def __init__(self, conf):\n",
    "        super(TweetModel, self).__init__(conf)\n",
    "        self.roberta = transformers.RobertaModel.from_pretrained(config.MODEL_NAME, config=conf)\n",
    "        self.drop_out = nn.Dropout(0.3)\n",
    "        self.conv1d_128_0 = nn.Conv1d(768, 128, kernel_size=2, \\\n",
    "                           stride=1, padding=0, dilation=1, groups=1, \\\n",
    "                           bias=True, padding_mode='zeros')\n",
    "        self.conv1d_128_1 = nn.Conv1d(768, 128, kernel_size=2, \\\n",
    "                           stride=1, padding=0, dilation=1, groups=1, \\\n",
    "                           bias=True, padding_mode='zeros')\n",
    "        self.conv1d_64_0 = nn.Conv1d(128, 64, kernel_size=2, \\\n",
    "                           stride=1, padding=0, dilation=1, groups=1, \\\n",
    "                           bias=True, padding_mode='zeros')\n",
    "        self.conv1d_64_1 = nn.Conv1d(128, 64, kernel_size=2, \\\n",
    "                           stride=1, padding=0, dilation=1, groups=1, \\\n",
    "                           bias=True, padding_mode='zeros')\n",
    "        \n",
    "        self.relu = nn.LeakyReLU()\n",
    "        \n",
    "        self.l0 = nn.Linear(64, 1)\n",
    "        self.l1 = nn.Linear(64, 1)\n",
    "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
    "        torch.nn.init.normal_(self.l1.weight, std=0.02)\n",
    "        \n",
    "        self.pad = nn.ConstantPad1d((0, 1), 0)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        out, _ = self.roberta( #\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        #out =  torch.cat((out[-1], out[-2]), dim=-1)\n",
    "        out1 = out.permute(0, 2, 1)\n",
    "        out1 = self.drop_out(out1)\n",
    "        #print(out.shape)\n",
    "        out1 = self.pad(out1)\n",
    "        #print(out.shape)\n",
    "        out1 = self.conv1d_128_0(out1)\n",
    "        #print(out.shape)\n",
    "        out1 = self.relu(out1)\n",
    "        #print(out.shape)\n",
    "        out1 = self.pad(out1)\n",
    "        out1 = self.conv1d_64_0(out1)\n",
    "        #print(out.shape)\n",
    "        #out = out.flatten()\n",
    "        out1 = out1.permute(0, 2, 1)\n",
    "        #print(out.shape)\n",
    "        start_logits = self.l0(out1)\n",
    "        #print(logits.shape)\n",
    "        \n",
    "        #out =  torch.cat((out[-1], out[-2]), dim=-1)\n",
    "        out2 = out.permute(0, 2, 1)\n",
    "        out2 = self.drop_out(out2)\n",
    "        #print(out.shape)\n",
    "        out2 = self.pad(out2)\n",
    "        #print(out.shape)\n",
    "        out2 = self.conv1d_128_1(out2)\n",
    "        #print(out.shape)\n",
    "        out2 = self.relu(out2)\n",
    "        #print(out.shape)\n",
    "        out2 = self.pad(out2)\n",
    "        out2 = self.conv1d_64_1(out2)\n",
    "        #print(out.shape)\n",
    "        #out = out.flatten()\n",
    "        out2 = out2.permute(0, 2, 1)\n",
    "        #print(out.shape)\n",
    "        end_logits = self.l1(out2)\n",
    "        #print(logits.shape)\n",
    "        \n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
    "    tweet = \" \" + \" \".join(str(tweet).split())\n",
    "    selected_text = \" \" + \" \".join(str(selected_text).split())\n",
    "\n",
    "    len_st = len(selected_text) - 1\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st - 1\n",
    "            break\n",
    "\n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 != None and idx1 != None:\n",
    "        for ct in range(idx0, idx1 + 1):\n",
    "            char_targets[ct] = 1\n",
    "    \n",
    "    tok_tweet = tokenizer.encode(tweet)\n",
    "    input_ids_orig = tok_tweet.ids\n",
    "    tweet_offsets = tok_tweet.offsets\n",
    "    \n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
    "        if sum(char_targets[offset1: offset2]) > 0:\n",
    "            target_idx.append(j)\n",
    "    \n",
    "    targets_start = target_idx[0]\n",
    "    targets_end = target_idx[-1]\n",
    "\n",
    "    sentiment_id = {\n",
    "        'positive': 1313,\n",
    "        'negative': 2430,\n",
    "        'neutral': 7974\n",
    "    }\n",
    "    \n",
    "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
    "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n",
    "    mask = [1] * len(token_type_ids)\n",
    "    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
    "    targets_start += 4\n",
    "    targets_end += 4\n",
    "\n",
    "    padding_length = max_len - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([1] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
    "    \n",
    "    return {\n",
    "        'ids': input_ids,\n",
    "        'mask': mask,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'targets_start': targets_start,\n",
    "        'targets_end': targets_end,\n",
    "        'orig_tweet': tweet,\n",
    "        'orig_selected': selected_text,\n",
    "        'sentiment': sentiment,\n",
    "        'offsets': tweet_offsets\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TweetDataset:\n",
    "    def __init__(self, tweet, sentiment, selected_text):\n",
    "        self.tweet = tweet\n",
    "        self.sentiment = sentiment\n",
    "        self.selected_text = selected_text\n",
    "        self.tokenizer = config.TOKENIZER\n",
    "        self.max_len = config.MAX_LEN\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = process_data(\n",
    "            self.tweet[item], \n",
    "            self.selected_text[item], \n",
    "            self.sentiment[item],\n",
    "            self.tokenizer,\n",
    "            self.max_len\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
    "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
    "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
    "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
    "            'orig_tweet': data[\"orig_tweet\"],\n",
    "            'orig_selected': data[\"orig_selected\"],\n",
    "            'sentiment': data[\"sentiment\"],\n",
    "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def calculate_jaccard_score(\n",
    "    original_tweet, \n",
    "    target_string, \n",
    "    sentiment_val, \n",
    "    idx_start, \n",
    "    idx_end, \n",
    "    offsets,\n",
    "    verbose=False):\n",
    "    \n",
    "    if idx_end < idx_start:\n",
    "        idx_end = idx_start\n",
    "    \n",
    "    filtered_output  = \"\"\n",
    "    for ix in range(idx_start, idx_end + 1):\n",
    "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
    "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
    "            filtered_output += \" \"\n",
    "\n",
    "    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n",
    "        filtered_output = original_tweet\n",
    "\n",
    "    jac = jaccard(target_string.strip(), filtered_output.strip())\n",
    "    return jac, filtered_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value.\n",
    "\n",
    "    Examples::\n",
    "        >>> # Initialize a meter to record loss\n",
    "        >>> losses = AverageMeter()\n",
    "        >>> # Update meter after every minibatch update\n",
    "        >>> losses.update(loss_value, batch_size)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, model_path):\n",
    "\n",
    "        score = val_loss#-val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, model_path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, model_path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, model_path):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(logits, positions):\n",
    "    start_logits, end_logits = logits\n",
    "    start_positions, end_positions = positions\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    start_loss = loss_fct(start_logits, start_positions)\n",
    "    end_loss = loss_fct(end_logits, end_positions)\n",
    "    total_loss = (start_loss + end_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    jaccards = AverageMeter()\n",
    "\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    \n",
    "    for bi, d in enumerate(tk0):\n",
    "\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        sentiment = d[\"sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        offsets = d[\"offsets\"]\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets_start = targets_start.to(device, dtype=torch.long)\n",
    "        targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs_start, outputs_end = model(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
    "        loss = loss #/ config.GRAD_ACC_STEPS\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "        jaccard_scores = []\n",
    "        for px, tweet in enumerate(orig_tweet):\n",
    "            selected_tweet = orig_selected[px]\n",
    "            tweet_sentiment = sentiment[px]\n",
    "            jaccard_score, _ = calculate_jaccard_score(\n",
    "                original_tweet=tweet,\n",
    "                target_string=selected_tweet,\n",
    "                sentiment_val=tweet_sentiment,\n",
    "                idx_start=np.argmax(outputs_start[px, :]),\n",
    "                idx_end=np.argmax(outputs_end[px, :]),\n",
    "                offsets=offsets[px]\n",
    "            )\n",
    "            jaccard_scores.append(jaccard_score)\n",
    "\n",
    "        jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "        losses.update(loss.item(), ids.size(0))\n",
    "        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def eval_fn(data_loader, model, device):\n",
    "#     model.eval()\n",
    "#     losses = AverageMeter()\n",
    "#     jaccards = AverageMeter()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "#         for bi, d in enumerate(tk0):\n",
    "#             ids = d[\"ids\"]\n",
    "#             token_type_ids = d[\"token_type_ids\"]\n",
    "#             mask = d[\"mask\"]\n",
    "#             sentiment = d[\"sentiment\"]\n",
    "#             orig_selected = d[\"orig_selected\"]\n",
    "#             orig_tweet = d[\"orig_tweet\"]\n",
    "#             targets_start = d[\"targets_start\"]\n",
    "#             targets_end = d[\"targets_end\"]\n",
    "#             offsets = d[\"offsets\"].numpy()\n",
    "\n",
    "#             ids = ids.to(device, dtype=torch.long)\n",
    "#             token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "#             mask = mask.to(device, dtype=torch.long)\n",
    "#             targets_start = targets_start.to(device, dtype=torch.long)\n",
    "#             targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "#             outputs_start, outputs_end = model(\n",
    "#                 ids=ids,\n",
    "#                 mask=mask,\n",
    "#                 token_type_ids=token_type_ids\n",
    "#             )\n",
    "#             loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
    "#             outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "#             outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "#             jaccard_scores = []\n",
    "#             for px, tweet in enumerate(orig_tweet):\n",
    "#                 selected_tweet = orig_selected[px]\n",
    "#                 tweet_sentiment = sentiment[px]\n",
    "#                 jaccard_score, _ = calculate_jaccard_score(\n",
    "#                     original_tweet=tweet,\n",
    "#                     target_string=selected_tweet,\n",
    "#                     sentiment_val=tweet_sentiment,\n",
    "#                     idx_start=np.argmax(outputs_start[px, :]),\n",
    "#                     idx_end=np.argmax(outputs_end[px, :]),\n",
    "#                     offsets=offsets[px]\n",
    "#                 )\n",
    "#                 jaccard_scores.append(jaccard_score)\n",
    "\n",
    "#             jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "#             losses.update(loss.item(), ids.size(0))\n",
    "#             tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n",
    "    \n",
    "#     print(f\"Jaccard = {jaccards.avg}\")\n",
    "#     return jaccards.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def run(fold):\n",
    "#     dfx = pd.read_csv(config.TRAINING_FILE)\n",
    "\n",
    "#     df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n",
    "#     df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
    "# #     df_train = df_train.head(100)\n",
    "# #     df_valid = df_train\n",
    "# #     #\n",
    "    \n",
    "#     train_dataset = TweetDataset(\n",
    "#         tweet=df_train.text.values,\n",
    "#         sentiment=df_train.sentiment.values,\n",
    "#         selected_text=df_train.selected_text.values\n",
    "#     )\n",
    "\n",
    "#     train_data_loader = torch.utils.data.DataLoader(\n",
    "#         train_dataset,\n",
    "#         batch_size=config.TRAIN_BATCH_SIZE,\n",
    "#         num_workers=4\n",
    "#     )\n",
    "\n",
    "#     valid_dataset = TweetDataset(\n",
    "#         tweet=df_valid.text.values,\n",
    "#         sentiment=df_valid.sentiment.values,\n",
    "#         selected_text=df_valid.selected_text.values\n",
    "#     )\n",
    "\n",
    "#     valid_data_loader = torch.utils.data.DataLoader(\n",
    "#         valid_dataset,\n",
    "#         batch_size=config.VALID_BATCH_SIZE,\n",
    "#         num_workers=2\n",
    "#     )\n",
    "\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     model_config = transformers.RobertaConfig.from_pretrained(config.MODEL_NAME)\n",
    "#     #model_config.output_hidden_states = True\n",
    "#     model = TweetModel(conf=model_config)\n",
    "#     model.to(device)\n",
    "\n",
    "#     num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "#     param_optimizer = list(model.named_parameters())\n",
    "#     no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "#     optimizer_parameters = [\n",
    "#         {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "#         {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "#     ]\n",
    "#     optimizer = AdamW(optimizer_parameters, lr=config.LEARNING_RATE)\n",
    "#     scheduler = get_linear_schedule_with_warmup(\n",
    "#         optimizer, \n",
    "#         num_warmup_steps=0, \n",
    "#         num_training_steps=num_train_steps\n",
    "#     )\n",
    "\n",
    "#     es = EarlyStopping(patience=100)#, mode=\"max\")\n",
    "#     print(f\"Training is Starting for fold={fold}\")\n",
    "    \n",
    "#     # I'm training only for 3 epochs even though I specified 5!!!\n",
    "#     for epoch in range(config.EPOCHS):\n",
    "#         train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n",
    "#         jaccard = eval_fn(valid_data_loader, model, device)\n",
    "#         print(f\"Jaccard Score = {jaccard}\")\n",
    "#         es(jaccard, model, model_path=f\"model_{fold}-exp3.bin\")\n",
    "#         if es.early_stop:\n",
    "#             print(\"Early stopping\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [
     25,
     400,
     421,
     442,
     492
    ]
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import torch\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "\n",
    "    IS_AMP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    import logging\n",
    "\n",
    "    logging.basicConfig()\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.warning(\n",
    "        \"To enable mixed precision training, please install `apex`. \"\n",
    "        \"Or you can re-install this package by the following command:\\n\"\n",
    "        '  pip install torch-lr-finder -v --global-option=\"amp\"'\n",
    "    )\n",
    "    IS_AMP_AVAILABLE = False\n",
    "    del logging\n",
    "\n",
    "\n",
    "class LRFinder(object):\n",
    "    \"\"\"Learning rate range test.\n",
    "    The learning rate range test increases the learning rate in a pre-training run\n",
    "    between two boundaries in a linear or exponential manner. It provides valuable\n",
    "    information on how well the network can be trained over a range of learning rates\n",
    "    and what is the optimal learning rate.\n",
    "    Arguments:\n",
    "        model (torch.nn.Module): wrapped model.\n",
    "        optimizer (torch.optim.Optimizer): wrapped optimizer where the defined learning\n",
    "            is assumed to be the lower boundary of the range test.\n",
    "        criterion (torch.nn.Module): wrapped loss function.\n",
    "        device (str or torch.device, optional): a string (\"cpu\" or \"cuda\") with an\n",
    "            optional ordinal for the device type (e.g. \"cuda:X\", where is the ordinal).\n",
    "            Alternatively, can be an object representing the device on which the\n",
    "            computation will take place. Default: None, uses the same device as `model`.\n",
    "        memory_cache (boolean, optional): if this flag is set to True, `state_dict` of\n",
    "            model and optimizer will be cached in memory. Otherwise, they will be saved\n",
    "            to files under the `cache_dir`.\n",
    "        cache_dir (string, optional): path for storing temporary files. If no path is\n",
    "            specified, system-wide temporary directory is used. Notice that this\n",
    "            parameter will be ignored if `memory_cache` is True.\n",
    "    Example:\n",
    "        >>> lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\n",
    "        >>> lr_finder.range_test(dataloader, end_lr=100, num_iter=100)\n",
    "        >>> lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "        >>> lr_finder.reset() # to reset the model and optimizer to their initial state\n",
    "    Reference:\n",
    "    Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n",
    "    fastai/lr_find: https://github.com/fastai/fastai\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device=None,\n",
    "        memory_cache=True,\n",
    "        cache_dir=None,\n",
    "    ):\n",
    "        # Check if the optimizer is already attached to a scheduler\n",
    "        self.optimizer = optimizer\n",
    "        self._check_for_scheduler()\n",
    "\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.history = {\"lr\": [], \"loss\": []}\n",
    "        self.best_loss = None\n",
    "        self.memory_cache = memory_cache\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        # Save the original state of the model and optimizer so they can be restored if\n",
    "        # needed\n",
    "        self.model_device = next(self.model.parameters()).device\n",
    "        self.state_cacher = StateCacher(memory_cache, cache_dir=cache_dir)\n",
    "        self.state_cacher.store(\"model\", self.model.state_dict())\n",
    "        self.state_cacher.store(\"optimizer\", self.optimizer.state_dict())\n",
    "\n",
    "        # If device is None, use the same as the model\n",
    "        if device:\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = self.model_device\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Restores the model and optimizer to their initial states.\"\"\"\n",
    "\n",
    "        self.model.load_state_dict(self.state_cacher.retrieve(\"model\"))\n",
    "        self.optimizer.load_state_dict(self.state_cacher.retrieve(\"optimizer\"))\n",
    "        self.model.to(self.model_device)\n",
    "\n",
    "    def range_test(\n",
    "        self,\n",
    "        train_loader,\n",
    "        val_loader=None,\n",
    "        start_lr=None,\n",
    "        end_lr=10,\n",
    "        num_iter=100,\n",
    "        step_mode=\"exp\",\n",
    "        smooth_f=0.05,\n",
    "        diverge_th=5,\n",
    "        accumulation_steps=1,\n",
    "    ):\n",
    "        \"\"\"Performs the learning rate range test.\n",
    "        Arguments:\n",
    "            train_loader (torch.utils.data.DataLoader): the training set data laoder.\n",
    "            val_loader (torch.utils.data.DataLoader, optional): if `None` the range test\n",
    "                will only use the training loss. When given a data loader, the model is\n",
    "                evaluated after each iteration on that dataset and the evaluation loss\n",
    "                is used. Note that in this mode the test takes significantly longer but\n",
    "                generally produces more precise results. Default: None.\n",
    "            start_lr (float, optional): the starting learning rate for the range test.\n",
    "                Default: None (uses the learning rate from the optimizer).\n",
    "            end_lr (float, optional): the maximum learning rate to test. Default: 10.\n",
    "            num_iter (int, optional): the number of iterations over which the test\n",
    "                occurs. Default: 100.\n",
    "            step_mode (str, optional): one of the available learning rate policies,\n",
    "                linear or exponential (\"linear\", \"exp\"). Default: \"exp\".\n",
    "            smooth_f (float, optional): the loss smoothing factor within the [0, 1[\n",
    "                interval. Disabled if set to 0, otherwise the loss is smoothed using\n",
    "                exponential smoothing. Default: 0.05.\n",
    "            diverge_th (int, optional): the test is stopped when the loss surpasses the\n",
    "                threshold:  diverge_th * best_loss. Default: 5.\n",
    "            accumulation_steps (int, optional): steps for gradient accumulation. If it\n",
    "                is 1, gradients are not accumulated. Default: 1.\n",
    "        Example (fastai approach):\n",
    "            >>> lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\n",
    "            >>> lr_finder.range_test(dataloader, end_lr=100, num_iter=100)\n",
    "        Example (Leslie Smith's approach):\n",
    "            >>> lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\n",
    "            >>> lr_finder.range_test(trainloader, val_loader=val_loader, end_lr=1, num_iter=100, step_mode=\"linear\")\n",
    "        Gradient accumulation is supported; example:\n",
    "            >>> train_data = ...    # prepared dataset\n",
    "            >>> desired_bs, real_bs = 32, 4         # batch size\n",
    "            >>> accumulation_steps = desired_bs // real_bs     # required steps for accumulation\n",
    "            >>> dataloader = torch.utils.data.DataLoader(train_data, batch_size=real_bs, shuffle=True)\n",
    "            >>> acc_lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\n",
    "            >>> acc_lr_finder.range_test(dataloader, end_lr=10, num_iter=100, accumulation_steps=accumulation_steps)\n",
    "        Reference:\n",
    "        [Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed setups](\n",
    "        https://medium.com/huggingface/ec88c3e51255)\n",
    "        [thomwolf/gradient_accumulation](https://gist.github.com/thomwolf/ac7a7da6b1888c2eeac8ac8b9b05d3d3)\n",
    "        \"\"\"\n",
    "\n",
    "        # Reset test results\n",
    "        self.history = {\"lr\": [], \"loss\": []}\n",
    "        self.best_loss = None\n",
    "\n",
    "        # Move the model to the proper device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Check if the optimizer is already attached to a scheduler\n",
    "        self._check_for_scheduler()\n",
    "\n",
    "        # Set the starting learning rate\n",
    "        if start_lr:\n",
    "            self._set_learning_rate(start_lr)\n",
    "\n",
    "        # Initialize the proper learning rate policy\n",
    "        if step_mode.lower() == \"exp\":\n",
    "            lr_schedule = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
    "        elif step_mode.lower() == \"linear\":\n",
    "            lr_schedule = LinearLR(self.optimizer, end_lr, num_iter)\n",
    "        else:\n",
    "            raise ValueError(\"expected one of (exp, linear), got {}\".format(step_mode))\n",
    "\n",
    "        if smooth_f < 0 or smooth_f >= 1:\n",
    "            raise ValueError(\"smooth_f is outside the range [0, 1[\")\n",
    "\n",
    "        # Create an iterator to get data batch by batch\n",
    "        iter_wrapper = DataLoaderIterWrapper(train_loader)\n",
    "        for iteration in tqdm(range(num_iter)):\n",
    "            # Train on batch and retrieve loss\n",
    "            loss = self._train_batch(iter_wrapper, accumulation_steps)\n",
    "            if val_loader:\n",
    "                loss = self._validate(val_loader)\n",
    "\n",
    "            # Update the learning rate\n",
    "            lr_schedule.step()\n",
    "            self.history[\"lr\"].append(lr_schedule.get_lr()[0])\n",
    "\n",
    "            # Track the best loss and smooth it if smooth_f is specified\n",
    "            if iteration == 0:\n",
    "                self.best_loss = loss\n",
    "            else:\n",
    "                if smooth_f > 0:\n",
    "                    loss = smooth_f * loss + (1 - smooth_f) * self.history[\"loss\"][-1]\n",
    "                if loss < self.best_loss:\n",
    "                    self.best_loss = loss\n",
    "\n",
    "            # Check if the loss has diverged; if it has, stop the test\n",
    "            self.history[\"loss\"].append(loss)\n",
    "            if loss > diverge_th * self.best_loss:\n",
    "                print(\"Stopping early, the loss has diverged\")\n",
    "                break\n",
    "\n",
    "        print(\"Learning rate search finished. See the graph with {finder_name}.plot()\")\n",
    "\n",
    "    def _set_learning_rate(self, new_lrs):\n",
    "        if not isinstance(new_lrs, list):\n",
    "            new_lrs = [new_lrs] * len(self.optimizer.param_groups)\n",
    "        if len(new_lrs) != len(self.optimizer.param_groups):\n",
    "            raise ValueError(\n",
    "                \"Length of `new_lrs` is not equal to the number of parameter groups \"\n",
    "                + \"in the given optimizer\"\n",
    "            )\n",
    "\n",
    "        for param_group, new_lr in zip(self.optimizer.param_groups, new_lrs):\n",
    "            param_group[\"lr\"] = new_lr\n",
    "\n",
    "    def _check_for_scheduler(self):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            if \"initial_lr\" in param_group:\n",
    "                raise RuntimeError(\"Optimizer already has a scheduler attached to it\")\n",
    "\n",
    "    def _train_batch(self, iter_wrapper, accumulation_steps):\n",
    "        self.model.train()\n",
    "        total_loss = None  # for late initialization\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        for i in range(accumulation_steps):\n",
    "            \n",
    "            d = next(iter_wrapper)\n",
    "            device = 'cuda'\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            targets_start = d[\"targets_start\"]\n",
    "            targets_end = d[\"targets_end\"]\n",
    "            sentiment = d[\"sentiment\"]\n",
    "            orig_selected = d[\"orig_selected\"]\n",
    "            orig_tweet = d[\"orig_tweet\"]\n",
    "            targets_start = d[\"targets_start\"]\n",
    "            targets_end = d[\"targets_end\"]\n",
    "            offsets = d[\"offsets\"]\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets_start = targets_start.to(device, dtype=torch.long)\n",
    "            targets_end = targets_end.to(device, dtype=torch.long)\n",
    "            \n",
    "            labels = (targets_start, targets_end)\n",
    "            \n",
    "            #inputs, labels = self._move_to_device(inputs, labels)\n",
    "            # Forward pass\n",
    "            outputs = self.model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            loss = self.criterion(outputs, labels)\n",
    "\n",
    "            # Loss should be averaged in each step\n",
    "            loss /= accumulation_steps\n",
    "\n",
    "            # Backward pass\n",
    "            if IS_AMP_AVAILABLE and hasattr(self.optimizer, \"_amp_stash\"):\n",
    "                # For minor performance optimization, see also:\n",
    "                # https://nvidia.github.io/apex/advanced.html#gradient-accumulation-across-iterations\n",
    "                delay_unscale = ((i + 1) % accumulation_steps) != 0\n",
    "\n",
    "                with amp.scale_loss(\n",
    "                    loss, self.optimizer, delay_unscale=delay_unscale\n",
    "                ) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if total_loss is None:\n",
    "                total_loss = loss\n",
    "            else:\n",
    "                total_loss += loss\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return total_loss.item()\n",
    "\n",
    "    def _move_to_device(self, inputs, labels):\n",
    "        def move(obj, device):\n",
    "            if hasattr(obj, \"to\"):\n",
    "                return obj.to(device)\n",
    "            elif isinstance(obj, tuple):\n",
    "                return tuple(move(o, device) for o in obj)\n",
    "            elif isinstance(obj, list):\n",
    "                return [move(o, device) for o in obj]\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: move(o, device) for k, o in obj.items()}\n",
    "            else:\n",
    "                return obj\n",
    "\n",
    "        inputs = move(inputs, self.device)\n",
    "        labels = move(labels, self.device)\n",
    "        return inputs, labels\n",
    "\n",
    "    def _validate(self, dataloader):\n",
    "        # Set model to evaluation mode and disable gradient computation\n",
    "        running_loss = 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for d in dataloader:\n",
    "                \n",
    "                device = 'cuda'\n",
    "                ids = d[\"ids\"]\n",
    "                token_type_ids = d[\"token_type_ids\"]\n",
    "                mask = d[\"mask\"]\n",
    "                sentiment = d[\"sentiment\"]\n",
    "                orig_selected = d[\"orig_selected\"]\n",
    "                orig_tweet = d[\"orig_tweet\"]\n",
    "                targets_start = d[\"targets_start\"]\n",
    "                targets_end = d[\"targets_end\"]\n",
    "                offsets = d[\"offsets\"].numpy()\n",
    "                ids = ids.to(device, dtype=torch.long)\n",
    "                token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "                mask = mask.to(device, dtype=torch.long)\n",
    "                targets_start = targets_start.to(device, dtype=torch.long)\n",
    "                targets_end = targets_end.to(device, dtype=torch.long)\n",
    "            \n",
    "                # Move data to the correct device\n",
    "                #inputs, labels = self._move_to_device(inputs, labels)\n",
    "\n",
    "                if isinstance(inputs, tuple) or isinstance(inputs, list):\n",
    "                    batch_size = inputs[0].size(0)\n",
    "                else:\n",
    "                    batch_size = inputs.size(0)\n",
    "\n",
    "                # Forward pass and loss computation\n",
    "                outputs = self.model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item() * batch_size\n",
    "\n",
    "        return running_loss / len(dataloader.dataset)\n",
    "\n",
    "    def plot(self, skip_start=10, skip_end=5, log_lr=True, show_lr=None, ax=None):\n",
    "        \"\"\"Plots the learning rate range test.\n",
    "        Arguments:\n",
    "            skip_start (int, optional): number of batches to trim from the start.\n",
    "                Default: 10.\n",
    "            skip_end (int, optional): number of batches to trim from the start.\n",
    "                Default: 5.\n",
    "            log_lr (bool, optional): True to plot the learning rate in a logarithmic\n",
    "                scale; otherwise, plotted in a linear scale. Default: True.\n",
    "            show_lr (float, optional): if set, adds a vertical line to visualize the\n",
    "                specified learning rate. Default: None.\n",
    "            ax (matplotlib.axes.Axes, optional): the plot is created in the specified\n",
    "                matplotlib axes object and the figure is not be shown. If `None`, then\n",
    "                the figure and axes object are created in this method and the figure is\n",
    "                shown . Default: None.\n",
    "        Returns:\n",
    "            The matplotlib.axes.Axes object that contains the plot.\n",
    "        \"\"\"\n",
    "\n",
    "        if skip_start < 0:\n",
    "            raise ValueError(\"skip_start cannot be negative\")\n",
    "        if skip_end < 0:\n",
    "            raise ValueError(\"skip_end cannot be negative\")\n",
    "        if show_lr is not None and not isinstance(show_lr, float):\n",
    "            raise ValueError(\"show_lr must be float\")\n",
    "\n",
    "        # Get the data to plot from the history dictionary. Also, handle skip_end=0\n",
    "        # properly so the behaviour is the expected\n",
    "        lrs = self.history[\"lr\"]\n",
    "        losses = self.history[\"loss\"]\n",
    "        if skip_end == 0:\n",
    "            lrs = lrs[skip_start:]\n",
    "            losses = losses[skip_start:]\n",
    "        else:\n",
    "            lrs = lrs[skip_start:-skip_end]\n",
    "            losses = losses[skip_start:-skip_end]\n",
    "\n",
    "        # Create the figure and axes object if axes was not already given\n",
    "        fig = None\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "        # Plot loss as a function of the learning rate\n",
    "        ax.plot(lrs, losses)\n",
    "        if log_lr:\n",
    "            ax.set_xscale(\"log\")\n",
    "        ax.set_xlabel(\"Learning rate\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "\n",
    "        if show_lr is not None:\n",
    "            ax.axvline(x=show_lr, color=\"red\")\n",
    "\n",
    "        # Show only if the figure was created internally\n",
    "        if fig is not None:\n",
    "            plt.show()\n",
    "\n",
    "        return ax\n",
    "\n",
    "\n",
    "class LinearLR(_LRScheduler):\n",
    "    \"\"\"Linearly increases the learning rate between two boundaries over a number of\n",
    "    iterations.\n",
    "    Arguments:\n",
    "        optimizer (torch.optim.Optimizer): wrapped optimizer.\n",
    "        end_lr (float): the final learning rate.\n",
    "        num_iter (int): the number of iterations over which the test occurs.\n",
    "        last_epoch (int, optional): the index of last epoch. Default: -1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
    "        self.end_lr = end_lr\n",
    "        self.num_iter = num_iter\n",
    "        super(LinearLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        curr_iter = self.last_epoch + 1\n",
    "        r = curr_iter / self.num_iter\n",
    "        return [base_lr + r * (self.end_lr - base_lr) for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class ExponentialLR(_LRScheduler):\n",
    "    \"\"\"Exponentially increases the learning rate between two boundaries over a number of\n",
    "    iterations.\n",
    "    Arguments:\n",
    "        optimizer (torch.optim.Optimizer): wrapped optimizer.\n",
    "        end_lr (float): the final learning rate.\n",
    "        num_iter (int): the number of iterations over which the test occurs.\n",
    "        last_epoch (int, optional): the index of last epoch. Default: -1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
    "        self.end_lr = end_lr\n",
    "        self.num_iter = num_iter\n",
    "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        curr_iter = self.last_epoch + 1\n",
    "        r = curr_iter / self.num_iter\n",
    "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class StateCacher(object):\n",
    "    def __init__(self, in_memory, cache_dir=None):\n",
    "        self.in_memory = in_memory\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        if self.cache_dir is None:\n",
    "            import tempfile\n",
    "\n",
    "            self.cache_dir = tempfile.gettempdir()\n",
    "        else:\n",
    "            if not os.path.isdir(self.cache_dir):\n",
    "                raise ValueError(\"Given `cache_dir` is not a valid directory.\")\n",
    "\n",
    "        self.cached = {}\n",
    "\n",
    "    def store(self, key, state_dict):\n",
    "        if self.in_memory:\n",
    "            self.cached.update({key: copy.deepcopy(state_dict)})\n",
    "        else:\n",
    "            fn = os.path.join(self.cache_dir, \"state_{}_{}.pt\".format(key, id(self)))\n",
    "            self.cached.update({key: fn})\n",
    "            torch.save(state_dict, fn)\n",
    "\n",
    "    def retrieve(self, key):\n",
    "        if key not in self.cached:\n",
    "            raise KeyError(\"Target {} was not cached.\".format(key))\n",
    "\n",
    "        if self.in_memory:\n",
    "            return self.cached.get(key)\n",
    "        else:\n",
    "            fn = self.cached.get(key)\n",
    "            if not os.path.exists(fn):\n",
    "                raise RuntimeError(\n",
    "                    \"Failed to load state in {}. File doesn't exist anymore.\".format(fn)\n",
    "                )\n",
    "            state_dict = torch.load(fn, map_location=lambda storage, location: storage)\n",
    "            return state_dict\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Check whether there are unused cached files existing in `cache_dir` before\n",
    "        this instance being destroyed.\"\"\"\n",
    "\n",
    "        if self.in_memory:\n",
    "            return\n",
    "\n",
    "        for k in self.cached:\n",
    "            if os.path.exists(self.cached[k]):\n",
    "                os.remove(self.cached[k])\n",
    "\n",
    "\n",
    "class DataLoaderIterWrapper(object):\n",
    "    \"\"\"A wrapper for iterating `torch.utils.data.DataLoader` with the ability to reset\n",
    "    itself while `StopIteration` is raised.\"\"\"\n",
    "\n",
    "    def __init__(self, data_loader, auto_reset=True):\n",
    "        self.data_loader = data_loader\n",
    "        self.auto_reset = auto_reset\n",
    "        self._iterator = iter(data_loader)\n",
    "\n",
    "    def __next__(self):\n",
    "        # Get a new set of inputs and labels\n",
    "        try:\n",
    "            d = next(self._iterator)\n",
    "        except StopIteration:\n",
    "            if not self.auto_reset:\n",
    "                raise\n",
    "            self._iterator = iter(self.data_loader)\n",
    "            d = next(self._iterator)\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lr():\n",
    "    dfx = pd.read_csv(config.TRAINING_FILE)\n",
    "\n",
    "    fold = 0\n",
    "    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    train_dataset = TweetDataset(\n",
    "        tweet=df_train.text.values,\n",
    "        sentiment=df_train.sentiment.values,\n",
    "        selected_text=df_train.selected_text.values\n",
    "    )\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.TRAIN_BATCH_SIZE,\n",
    "        num_workers=4\n",
    "    )\n",
    "        \n",
    "    device = torch.device(\"cuda\")\n",
    "    model_config = transformers.RobertaConfig.from_pretrained(config.MODEL_NAME)\n",
    "    #model_config.output_hidden_states = True\n",
    "    model = TweetModel(conf=model_config)\n",
    "    model.to(device)\n",
    "    \n",
    "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_parameters, lr=1e-7)\n",
    "\n",
    "    \n",
    "    criterion = loss_fn\n",
    "    \n",
    "    lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "    lr_finder.range_test(train_data_loader, end_lr=0.01, num_iter=1000)\n",
    "    lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "    lr_finder.reset() # to reset the model and optimizer to their initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880d327b30184d4bb76e7a0c5de24131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
